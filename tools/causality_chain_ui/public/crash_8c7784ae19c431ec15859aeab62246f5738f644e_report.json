{
    "report": "https://syzkaller.appspot.com/bug?id=8c7784ae19c431ec15859aeab62246f5738f644e",
    "title": "general protection fault in try_grab_compound_head",
    "call": {
        "4405": {
            "name": "+0xb5",
            "parent_idx": 4404,
            "source_line": [
                {
                    "file": "./include/linux/mm.h",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/./include/linux/mm.h?id=3dbdb38e#n1121",
                    "code": [
                        "static inline enum zone_type page_zonenum(const struct page *page)",
                        "{",
                        "\tASSERT_EXCLUSIVE_BITS(page->flags, ZONES_MASK << ZONES_PGSHIFT);",
                        "\treturn (page->flags >> ZONES_PGSHIFT) & ZONES_MASK;",
                        "}"
                    ],
                    "start": 1118,
                    "highlight": 1121
                },
                {
                    "file": "./include/linux/mm.h",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/./include/linux/mm.h?id=3dbdb38e#n1140",
                    "code": [
                        "static inline bool is_zone_movable_page(const struct page *page)",
                        "{",
                        "\treturn page_zonenum(page) == ZONE_MOVABLE;",
                        "}"
                    ],
                    "start": 1138,
                    "highlight": 1140
                },
                {
                    "file": "./include/linux/mm.h",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/./include/linux/mm.h?id=3dbdb38e#n1556",
                    "code": [
                        "/* MIGRATE_CMA and ZONE_MOVABLE do not allow pin pages */",
                        "#ifdef CONFIG_MIGRATION",
                        "static inline bool is_pinnable_page(struct page *page)",
                        "{",
                        "\treturn !(is_zone_movable_page(page) || is_migrate_cma_page(page)) ||",
                        "\t\tis_zero_pfn(page_to_pfn(page));",
                        "}"
                    ],
                    "start": 1552,
                    "highlight": 1556
                },
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n126",
                    "code": [
                        "/*",
                        " * try_grab_compound_head() - attempt to elevate a page's refcount, by a",
                        " * flags-dependent amount.",
                        " *",
                        " * \"grab\" names in this file mean, \"look at flags to decide whether to use",
                        " * FOLL_PIN or FOLL_GET behavior, when incrementing the page's refcount.",
                        " *",
                        " * Either FOLL_PIN or FOLL_GET (or neither) must be set, but not both at the",
                        " * same time. (That's true throughout the get_user_pages*() and",
                        " * pin_user_pages*() APIs.) Cases:",
                        " *",
                        " *    FOLL_GET: page's refcount will be incremented by 1.",
                        " *    FOLL_PIN: page's refcount will be incremented by GUP_PIN_COUNTING_BIAS.",
                        " *",
                        " * Return: head page (with refcount appropriately incremented) for success, or",
                        " * NULL upon failure. If neither FOLL_GET nor FOLL_PIN was set, that's",
                        " * considered failure, and furthermore, a likely bug in the caller, so a warning",
                        " * is also emitted.",
                        " */",
                        "__maybe_unused struct page *try_grab_compound_head(struct page *page,",
                        "\t\t\t\t\t\t   int refs, unsigned int flags)",
                        "{",
                        "\tif (flags & FOLL_GET)",
                        "\t\treturn try_get_compound_head(page, refs);",
                        "\telse if (flags & FOLL_PIN) {",
                        "\t\tint orig_refs = refs;",
                        "",
                        "\t\t/*",
                        "\t\t * Can't do FOLL_LONGTERM + FOLL_PIN gup fast path if not in a",
                        "\t\t * right zone, so fail and let the caller fall back to the slow",
                        "\t\t * path.",
                        "\t\t */",
                        "\t\tif (unlikely((flags & FOLL_LONGTERM) &&",
                        "\t\t\t     !is_pinnable_page(page)))",
                        "\t\t\treturn NULL;",
                        "",
                        "\t\t/*",
                        "\t\t * CAUTION: Don't use compound_head() on the page before this",
                        "\t\t * point, the result won't be stable.",
                        "\t\t */",
                        "\t\tpage = try_get_compound_head(page, refs);",
                        "\t\tif (!page)",
                        "\t\t\treturn NULL;",
                        "",
                        "\t\t/*",
                        "\t\t * When pinning a compound page of order > 1 (which is what",
                        "\t\t * hpage_pincount_available() checks for), use an exact count to",
                        "\t\t * track it, via hpage_pincount_add/_sub().",
                        "\t\t *",
                        "\t\t * However, be sure to *also* increment the normal page refcount",
                        "\t\t * field at least once, so that the page really is pinned.",
                        "\t\t */",
                        "\t\tif (hpage_pincount_available(page))",
                        "\t\t\thpage_pincount_add(page, refs);",
                        "\t\telse",
                        "\t\t\tpage_ref_add(page, refs * (GUP_PIN_COUNTING_BIAS - 1));",
                        "",
                        "\t\tmod_node_page_state(page_pgdat(page), NR_FOLL_PIN_ACQUIRED,",
                        "\t\t\t\t    orig_refs);",
                        "",
                        "\t\treturn page;",
                        "\t}",
                        "",
                        "\tWARN_ON_ONCE(1);",
                        "\treturn NULL;",
                        "}"
                    ],
                    "start": 94,
                    "highlight": 126
                },
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n113",
                    "code": [
                        "/*",
                        " * try_grab_compound_head() - attempt to elevate a page's refcount, by a",
                        " * flags-dependent amount.",
                        " *",
                        " * \"grab\" names in this file mean, \"look at flags to decide whether to use",
                        " * FOLL_PIN or FOLL_GET behavior, when incrementing the page's refcount.",
                        " *",
                        " * Either FOLL_PIN or FOLL_GET (or neither) must be set, but not both at the",
                        " * same time. (That's true throughout the get_user_pages*() and",
                        " * pin_user_pages*() APIs.) Cases:",
                        " *",
                        " *    FOLL_GET: page's refcount will be incremented by 1.",
                        " *    FOLL_PIN: page's refcount will be incremented by GUP_PIN_COUNTING_BIAS.",
                        " *",
                        " * Return: head page (with refcount appropriately incremented) for success, or",
                        " * NULL upon failure. If neither FOLL_GET nor FOLL_PIN was set, that's",
                        " * considered failure, and furthermore, a likely bug in the caller, so a warning",
                        " * is also emitted.",
                        " */",
                        "__maybe_unused struct page *try_grab_compound_head(struct page *page,",
                        "\t\t\t\t\t\t   int refs, unsigned int flags)",
                        "{",
                        "\tif (flags & FOLL_GET)",
                        "\t\treturn try_get_compound_head(page, refs);",
                        "\telse if (flags & FOLL_PIN) {",
                        "\t\tint orig_refs = refs;",
                        "",
                        "\t\t/*",
                        "\t\t * Can't do FOLL_LONGTERM + FOLL_PIN gup fast path if not in a",
                        "\t\t * right zone, so fail and let the caller fall back to the slow",
                        "\t\t * path.",
                        "\t\t */",
                        "\t\tif (unlikely((flags & FOLL_LONGTERM) &&",
                        "\t\t\t     !is_pinnable_page(page)))",
                        "\t\t\treturn NULL;",
                        "",
                        "\t\t/*",
                        "\t\t * CAUTION: Don't use compound_head() on the page before this",
                        "\t\t * point, the result won't be stable.",
                        "\t\t */",
                        "\t\tpage = try_get_compound_head(page, refs);",
                        "\t\tif (!page)",
                        "\t\t\treturn NULL;",
                        "",
                        "\t\t/*",
                        "\t\t * When pinning a compound page of order > 1 (which is what",
                        "\t\t * hpage_pincount_available() checks for), use an exact count to",
                        "\t\t * track it, via hpage_pincount_add/_sub().",
                        "\t\t *",
                        "\t\t * However, be sure to *also* increment the normal page refcount",
                        "\t\t * field at least once, so that the page really is pinned.",
                        "\t\t */",
                        "\t\tif (hpage_pincount_available(page))",
                        "\t\t\thpage_pincount_add(page, refs);",
                        "\t\telse",
                        "\t\t\tpage_ref_add(page, refs * (GUP_PIN_COUNTING_BIAS - 1));",
                        "",
                        "\t\tmod_node_page_state(page_pgdat(page), NR_FOLL_PIN_ACQUIRED,",
                        "\t\t\t\t    orig_refs);",
                        "",
                        "\t\treturn page;",
                        "\t}",
                        "",
                        "\tWARN_ON_ONCE(1);",
                        "\treturn NULL;",
                        "}"
                    ],
                    "start": 94,
                    "highlight": 113
                }
            ],
            "ins_idx": 51,
            "addr": "0xffffffff811a1a65"
        },
        "4404": {
            "name": "try_grab_compound_head",
            "parent_idx": 3902,
            "source_line": [
                {
                    "file": "mm/hugetlb.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/hugetlb.c?id=3dbdb38e#n5248",
                    "code": [
                        "long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,",
                        "\t\t\t struct page **pages, struct vm_area_struct **vmas,",
                        "\t\t\t unsigned long *position, unsigned long *nr_pages,",
                        "\t\t\t long i, unsigned int flags, int *locked)",
                        "{",
                        "\tunsigned long pfn_offset;",
                        "\tunsigned long vaddr = *position;",
                        "\tunsigned long remainder = *nr_pages;",
                        "\tstruct hstate *h = hstate_vma(vma);",
                        "\tint err = -EFAULT, refs;",
                        "",
                        "\twhile (vaddr < vma->vm_end && remainder) {",
                        "\t\tpte_t *pte;",
                        "\t\tspinlock_t *ptl = NULL;",
                        "\t\tint absent;",
                        "\t\tstruct page *page;",
                        "",
                        "\t\t/*",
                        "\t\t * If we have a pending SIGKILL, don't keep faulting pages and",
                        "\t\t * potentially allocating memory.",
                        "\t\t */",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tremainder = 0;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t/*",
                        "\t\t * Some archs (sparc64, sh*) have multiple pte_ts to",
                        "\t\t * each hugepage.  We have to make sure we get the",
                        "\t\t * first, for the page indexing below to work.",
                        "\t\t *",
                        "\t\t * Note that page table lock is not held when pte is null.",
                        "\t\t */",
                        "\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h),",
                        "\t\t\t\t      huge_page_size(h));",
                        "\t\tif (pte)",
                        "\t\t\tptl = huge_pte_lock(h, mm, pte);",
                        "\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));",
                        "",
                        "\t\t/*",
                        "\t\t * When coredumping, it suits get_dump_page if we just return",
                        "\t\t * an error where there's an empty slot with no huge pagecache",
                        "\t\t * to back it.  This way, we avoid allocating a hugepage, and",
                        "\t\t * the sparse dumpfile avoids allocating disk blocks, but its",
                        "\t\t * huge holes still show up with zeroes where they need to be.",
                        "\t\t */",
                        "\t\tif (absent && (flags & FOLL_DUMP) &&",
                        "\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {",
                        "\t\t\tif (pte)",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\tremainder = 0;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t/*",
                        "\t\t * We need call hugetlb_fault for both hugepages under migration",
                        "\t\t * (in which case hugetlb_fault waits for the migration,) and",
                        "\t\t * hwpoisoned hugepages (in which case we need to prevent the",
                        "\t\t * caller from accessing to them.) In order to do this, we use",
                        "\t\t * here is_swap_pte instead of is_hugetlb_entry_migration and",
                        "\t\t * is_hugetlb_entry_hwpoisoned. This is because it simply covers",
                        "\t\t * both cases, and because we can't follow correct pages",
                        "\t\t * directly from any kind of swap entries.",
                        "\t\t */",
                        "\t\tif (absent || is_swap_pte(huge_ptep_get(pte)) ||",
                        "\t\t    ((flags & FOLL_WRITE) &&",
                        "\t\t      !huge_pte_write(huge_ptep_get(pte)))) {",
                        "\t\t\tvm_fault_t ret;",
                        "\t\t\tunsigned int fault_flags = 0;",
                        "",
                        "\t\t\tif (pte)",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\tif (flags & FOLL_WRITE)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_WRITE;",
                        "\t\t\tif (locked)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |",
                        "\t\t\t\t\tFAULT_FLAG_KILLABLE;",
                        "\t\t\tif (flags & FOLL_NOWAIT)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |",
                        "\t\t\t\t\tFAULT_FLAG_RETRY_NOWAIT;",
                        "\t\t\tif (flags & FOLL_TRIED) {",
                        "\t\t\t\t/*",
                        "\t\t\t\t * Note: FAULT_FLAG_ALLOW_RETRY and",
                        "\t\t\t\t * FAULT_FLAG_TRIED can co-exist",
                        "\t\t\t\t */",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_TRIED;",
                        "\t\t\t}",
                        "\t\t\tret = hugetlb_fault(mm, vma, vaddr, fault_flags);",
                        "\t\t\tif (ret & VM_FAULT_ERROR) {",
                        "\t\t\t\terr = vm_fault_to_errno(ret, flags);",
                        "\t\t\t\tremainder = 0;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t\tif (ret & VM_FAULT_RETRY) {",
                        "\t\t\t\tif (locked &&",
                        "\t\t\t\t    !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))",
                        "\t\t\t\t\t*locked = 0;",
                        "\t\t\t\t*nr_pages = 0;",
                        "\t\t\t\t/*",
                        "\t\t\t\t * VM_FAULT_RETRY must not return an",
                        "\t\t\t\t * error, it will return zero",
                        "\t\t\t\t * instead.",
                        "\t\t\t\t *",
                        "\t\t\t\t * No need to update \"position\" as the",
                        "\t\t\t\t * caller will not check it after",
                        "\t\t\t\t * *nr_pages is set to 0.",
                        "\t\t\t\t */",
                        "\t\t\t\treturn i;",
                        "\t\t\t}",
                        "\t\t\tcontinue;",
                        "\t\t}",
                        "",
                        "\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;",
                        "\t\tpage = pte_page(huge_ptep_get(pte));",
                        "",
                        "\t\t/*",
                        "\t\t * If subpage information not requested, update counters",
                        "\t\t * and skip the same_page loop below.",
                        "\t\t */",
                        "\t\tif (!pages && !vmas && !pfn_offset &&",
                        "\t\t    (vaddr + huge_page_size(h) < vma->vm_end) &&",
                        "\t\t    (remainder >= pages_per_huge_page(h))) {",
                        "\t\t\tvaddr += huge_page_size(h);",
                        "\t\t\tremainder -= pages_per_huge_page(h);",
                        "\t\t\ti += pages_per_huge_page(h);",
                        "\t\t\tspin_unlock(ptl);",
                        "\t\t\tcontinue;",
                        "\t\t}",
                        "",
                        "\t\trefs = min3(pages_per_huge_page(h) - pfn_offset,",
                        "\t\t\t    (vma->vm_end - vaddr) >> PAGE_SHIFT, remainder);",
                        "",
                        "\t\tif (pages || vmas)",
                        "\t\t\trecord_subpages_vmas(mem_map_offset(page, pfn_offset),",
                        "\t\t\t\t\t     vma, refs,",
                        "\t\t\t\t\t     likely(pages) ? pages + i : NULL,",
                        "\t\t\t\t\t     vmas ? vmas + i : NULL);",
                        "",
                        "\t\tif (pages) {",
                        "\t\t\t/*",
                        "\t\t\t * try_grab_compound_head() should always succeed here,",
                        "\t\t\t * because: a) we hold the ptl lock, and b) we've just",
                        "\t\t\t * checked that the huge page is present in the page",
                        "\t\t\t * tables. If the huge page is present, then the tail",
                        "\t\t\t * pages must also be present. The ptl prevents the",
                        "\t\t\t * head page and tail pages from being rearranged in",
                        "\t\t\t * any way. So this page must be available at this",
                        "\t\t\t * point, unless the page refcount overflowed:",
                        "\t\t\t */",
                        "\t\t\tif (WARN_ON_ONCE(!try_grab_compound_head(pages[i],",
                        "\t\t\t\t\t\t\t\t refs,",
                        "\t\t\t\t\t\t\t\t flags))) {",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\t\tremainder = 0;",
                        "\t\t\t\terr = -ENOMEM;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t}",
                        "",
                        "\t\tvaddr += (refs << PAGE_SHIFT);",
                        "\t\tremainder -= refs;",
                        "\t\ti += refs;",
                        "",
                        "\t\tspin_unlock(ptl);",
                        "\t}",
                        "\t*nr_pages = remainder;",
                        "\t/*",
                        "\t * setting position is actually required only if remainder is",
                        "\t * not zero but it's faster not to add a \"if (remainder)\"",
                        "\t * branch.",
                        "\t */",
                        "\t*position = vaddr;",
                        "",
                        "\treturn i ? i : err;",
                        "}"
                    ],
                    "start": 5099,
                    "highlight": 5248
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff811d8a27"
        },
        "3902": {
            "name": "follow_hugetlb_page",
            "parent_idx": 3868,
            "source_line": [
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n1137",
                    "code": [
                        "/**",
                        " * __get_user_pages() - pin user pages in memory",
                        " * @mm:\t\tmm_struct of target mm",
                        " * @start:\tstarting user address",
                        " * @nr_pages:\tnumber of pages from start to pin",
                        " * @gup_flags:\tflags modifying pin behaviour",
                        " * @pages:\tarray that receives pointers to the pages pinned.",
                        " *\t\tShould be at least nr_pages long. Or NULL, if caller",
                        " *\t\tonly intends to ensure the pages are faulted in.",
                        " * @vmas:\tarray of pointers to vmas corresponding to each page.",
                        " *\t\tOr NULL if the caller does not require them.",
                        " * @locked:     whether we're still with the mmap_lock held",
                        " *",
                        " * Returns either number of pages pinned (which may be less than the",
                        " * number requested), or an error. Details about the return value:",
                        " *",
                        " * -- If nr_pages is 0, returns 0.",
                        " * -- If nr_pages is >0, but no pages were pinned, returns -errno.",
                        " * -- If nr_pages is >0, and some pages were pinned, returns the number of",
                        " *    pages pinned. Again, this may be less than nr_pages.",
                        " * -- 0 return value is possible when the fault would need to be retried.",
                        " *",
                        " * The caller is responsible for releasing returned @pages, via put_page().",
                        " *",
                        " * @vmas are valid only as long as mmap_lock is held.",
                        " *",
                        " * Must be called with mmap_lock held.  It may be released.  See below.",
                        " *",
                        " * __get_user_pages walks a process's page tables and takes a reference to",
                        " * each struct page that each user address corresponds to at a given",
                        " * instant. That is, it takes the page that would be accessed if a user",
                        " * thread accesses the given user virtual address at that instant.",
                        " *",
                        " * This does not guarantee that the page exists in the user mappings when",
                        " * __get_user_pages returns, and there may even be a completely different",
                        " * page there in some cases (eg. if mmapped pagecache has been invalidated",
                        " * and subsequently re faulted). However it does guarantee that the page",
                        " * won't be freed completely. And mostly callers simply care that the page",
                        " * contains data that was valid *at some point in time*. Typically, an IO",
                        " * or similar operation cannot guarantee anything stronger anyway because",
                        " * locks can't be held over the syscall boundary.",
                        " *",
                        " * If @gup_flags & FOLL_WRITE == 0, the page must not be written to. If",
                        " * the page is written to, set_page_dirty (or set_page_dirty_lock, as",
                        " * appropriate) must be called after the page is finished with, and",
                        " * before put_page is called.",
                        " *",
                        " * If @locked != NULL, *@locked will be set to 0 when mmap_lock is",
                        " * released by an up_read().  That can happen if @gup_flags does not",
                        " * have FOLL_NOWAIT.",
                        " *",
                        " * A caller using such a combination of @locked and @gup_flags",
                        " * must therefore hold the mmap_lock for reading only, and recognize",
                        " * when it's been released.  Otherwise, it must be held for either",
                        " * reading or writing and will not be released.",
                        " *",
                        " * In most cases, get_user_pages or get_user_pages_fast should be used",
                        " * instead of __get_user_pages. __get_user_pages should be used only if",
                        " * you need some special @gup_flags.",
                        " */",
                        "static long __get_user_pages(struct mm_struct *mm,",
                        "\t\tunsigned long start, unsigned long nr_pages,",
                        "\t\tunsigned int gup_flags, struct page **pages,",
                        "\t\tstruct vm_area_struct **vmas, int *locked)",
                        "{",
                        "\tlong ret = 0, i = 0;",
                        "\tstruct vm_area_struct *vma = NULL;",
                        "\tstruct follow_page_context ctx = { NULL };",
                        "",
                        "\tif (!nr_pages)",
                        "\t\treturn 0;",
                        "",
                        "\tstart = untagged_addr(start);",
                        "",
                        "\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));",
                        "",
                        "\t/*",
                        "\t * If FOLL_FORCE is set then do not force a full fault as the hinting",
                        "\t * fault information is unrelated to the reference behaviour of a task",
                        "\t * using the address space",
                        "\t */",
                        "\tif (!(gup_flags & FOLL_FORCE))",
                        "\t\tgup_flags |= FOLL_NUMA;",
                        "",
                        "\tdo {",
                        "\t\tstruct page *page;",
                        "\t\tunsigned int foll_flags = gup_flags;",
                        "\t\tunsigned int page_increm;",
                        "",
                        "\t\t/* first iteration or cross vma bound */",
                        "\t\tif (!vma || start >= vma->vm_end) {",
                        "\t\t\tvma = find_extend_vma(mm, start);",
                        "\t\t\tif (!vma && in_gate_area(mm, start)) {",
                        "\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,",
                        "\t\t\t\t\t\tgup_flags, &vma,",
                        "\t\t\t\t\t\tpages ? &pages[i] : NULL);",
                        "\t\t\t\tif (ret)",
                        "\t\t\t\t\tgoto out;",
                        "\t\t\t\tctx.page_mask = 0;",
                        "\t\t\t\tgoto next_page;",
                        "\t\t\t}",
                        "",
                        "\t\t\tif (!vma) {",
                        "\t\t\t\tret = -EFAULT;",
                        "\t\t\t\tgoto out;",
                        "\t\t\t}",
                        "\t\t\tret = check_vma_flags(vma, gup_flags);",
                        "\t\t\tif (ret)",
                        "\t\t\t\tgoto out;",
                        "",
                        "\t\t\tif (is_vm_hugetlb_page(vma)) {",
                        "\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,",
                        "\t\t\t\t\t\t&start, &nr_pages, i,",
                        "\t\t\t\t\t\tgup_flags, locked);",
                        "\t\t\t\tif (locked && *locked == 0) {",
                        "\t\t\t\t\t/*",
                        "\t\t\t\t\t * We've got a VM_FAULT_RETRY",
                        "\t\t\t\t\t * and we've lost mmap_lock.",
                        "\t\t\t\t\t * We must stop here.",
                        "\t\t\t\t\t */",
                        "\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);",
                        "\t\t\t\t\tBUG_ON(ret != 0);",
                        "\t\t\t\t\tgoto out;",
                        "\t\t\t\t}",
                        "\t\t\t\tcontinue;",
                        "\t\t\t}",
                        "\t\t}",
                        "retry:",
                        "\t\t/*",
                        "\t\t * If we have a pending SIGKILL, don't keep faulting pages and",
                        "\t\t * potentially allocating memory.",
                        "\t\t */",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tret = -EINTR;",
                        "\t\t\tgoto out;",
                        "\t\t}",
                        "\t\tcond_resched();",
                        "",
                        "\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);",
                        "\t\tif (!page) {",
                        "\t\t\tret = faultin_page(vma, start, &foll_flags, locked);",
                        "\t\t\tswitch (ret) {",
                        "\t\t\tcase 0:",
                        "\t\t\t\tgoto retry;",
                        "\t\t\tcase -EBUSY:",
                        "\t\t\t\tret = 0;",
                        "\t\t\t\tfallthrough;",
                        "\t\t\tcase -EFAULT:",
                        "\t\t\tcase -ENOMEM:",
                        "\t\t\tcase -EHWPOISON:",
                        "\t\t\t\tgoto out;",
                        "\t\t\tcase -ENOENT:",
                        "\t\t\t\tgoto next_page;",
                        "\t\t\t}",
                        "\t\t\tBUG();",
                        "\t\t} else if (PTR_ERR(page) == -EEXIST) {",
                        "\t\t\t/*",
                        "\t\t\t * Proper page table entry exists, but no corresponding",
                        "\t\t\t * struct page.",
                        "\t\t\t */",
                        "\t\t\tgoto next_page;",
                        "\t\t} else if (IS_ERR(page)) {",
                        "\t\t\tret = PTR_ERR(page);",
                        "\t\t\tgoto out;",
                        "\t\t}",
                        "\t\tif (pages) {",
                        "\t\t\tpages[i] = page;",
                        "\t\t\tflush_anon_page(vma, page, start);",
                        "\t\t\tflush_dcache_page(page);",
                        "\t\t\tctx.page_mask = 0;",
                        "\t\t}",
                        "next_page:",
                        "\t\tif (vmas) {",
                        "\t\t\tvmas[i] = vma;",
                        "\t\t\tctx.page_mask = 0;",
                        "\t\t}",
                        "\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);",
                        "\t\tif (page_increm > nr_pages)",
                        "\t\t\tpage_increm = nr_pages;",
                        "\t\ti += page_increm;",
                        "\t\tstart += page_increm * PAGE_SIZE;",
                        "\t\tnr_pages -= page_increm;",
                        "\t} while (nr_pages);",
                        "out:",
                        "\tif (ctx.pgmap)",
                        "\t\tput_dev_pagemap(ctx.pgmap);",
                        "\treturn i ? i : ret;",
                        "}"
                    ],
                    "start": 1026,
                    "highlight": 1137
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff811a226e"
        },
        "3868": {
            "name": "__get_user_pages",
            "parent_idx": 3857,
            "source_line": [
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n1352",
                    "code": [
                        "/*",
                        " * Please note that this function, unlike __get_user_pages will not",
                        " * return 0 for nr_pages > 0 without FOLL_NOWAIT",
                        " */",
                        "static __always_inline long __get_user_pages_locked(struct mm_struct *mm,",
                        "\t\t\t\t\t\tunsigned long start,",
                        "\t\t\t\t\t\tunsigned long nr_pages,",
                        "\t\t\t\t\t\tstruct page **pages,",
                        "\t\t\t\t\t\tstruct vm_area_struct **vmas,",
                        "\t\t\t\t\t\tint *locked,",
                        "\t\t\t\t\t\tunsigned int flags)",
                        "{",
                        "\tlong ret, pages_done;",
                        "\tbool lock_dropped;",
                        "",
                        "\tif (locked) {",
                        "\t\t/* if VM_FAULT_RETRY can be returned, vmas become invalid */",
                        "\t\tBUG_ON(vmas);",
                        "\t\t/* check caller initialized locked */",
                        "\t\tBUG_ON(*locked != 1);",
                        "\t}",
                        "",
                        "\tif (flags & FOLL_PIN)",
                        "\t\tmm_set_has_pinned_flag(&mm->flags);",
                        "",
                        "\t/*",
                        "\t * FOLL_PIN and FOLL_GET are mutually exclusive. Traditional behavior",
                        "\t * is to set FOLL_GET if the caller wants pages[] filled in (but has",
                        "\t * carelessly failed to specify FOLL_GET), so keep doing that, but only",
                        "\t * for FOLL_GET, not for the newer FOLL_PIN.",
                        "\t *",
                        "\t * FOLL_PIN always expects pages to be non-null, but no need to assert",
                        "\t * that here, as any failures will be obvious enough.",
                        "\t */",
                        "\tif (pages && !(flags & FOLL_PIN))",
                        "\t\tflags |= FOLL_GET;",
                        "",
                        "\tpages_done = 0;",
                        "\tlock_dropped = false;",
                        "\tfor (;;) {",
                        "\t\tret = __get_user_pages(mm, start, nr_pages, flags, pages,",
                        "\t\t\t\t       vmas, locked);",
                        "\t\tif (!locked)",
                        "\t\t\t/* VM_FAULT_RETRY couldn't trigger, bypass */",
                        "\t\t\treturn ret;",
                        "",
                        "\t\t/* VM_FAULT_RETRY cannot return errors */",
                        "\t\tif (!*locked) {",
                        "\t\t\tBUG_ON(ret < 0);",
                        "\t\t\tBUG_ON(ret >= nr_pages);",
                        "\t\t}",
                        "",
                        "\t\tif (ret > 0) {",
                        "\t\t\tnr_pages -= ret;",
                        "\t\t\tpages_done += ret;",
                        "\t\t\tif (!nr_pages)",
                        "\t\t\t\tbreak;",
                        "\t\t}",
                        "\t\tif (*locked) {",
                        "\t\t\t/*",
                        "\t\t\t * VM_FAULT_RETRY didn't trigger or it was a",
                        "\t\t\t * FOLL_NOWAIT.",
                        "\t\t\t */",
                        "\t\t\tif (!pages_done)",
                        "\t\t\t\tpages_done = ret;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "\t\t/*",
                        "\t\t * VM_FAULT_RETRY triggered, so seek to the faulting offset.",
                        "\t\t * For the prefault case (!pages) we only update counts.",
                        "\t\t */",
                        "\t\tif (likely(pages))",
                        "\t\t\tpages += ret;",
                        "\t\tstart += ret << PAGE_SHIFT;",
                        "\t\tlock_dropped = true;",
                        "",
                        "retry:",
                        "\t\t/*",
                        "\t\t * Repeat on the address that fired VM_FAULT_RETRY",
                        "\t\t * with both FAULT_FLAG_ALLOW_RETRY and",
                        "\t\t * FAULT_FLAG_TRIED.  Note that GUP can be interrupted",
                        "\t\t * by fatal signals, so we need to check it before we",
                        "\t\t * start trying again otherwise it can loop forever.",
                        "\t\t */",
                        "",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tif (!pages_done)",
                        "\t\t\t\tpages_done = -EINTR;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\tret = mmap_read_lock_killable(mm);",
                        "\t\tif (ret) {",
                        "\t\t\tBUG_ON(ret > 0);",
                        "\t\t\tif (!pages_done)",
                        "\t\t\t\tpages_done = ret;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t*locked = 1;",
                        "\t\tret = __get_user_pages(mm, start, 1, flags | FOLL_TRIED,",
                        "\t\t\t\t       pages, NULL, locked);",
                        "\t\tif (!*locked) {",
                        "\t\t\t/* Continue to retry until we succeeded */",
                        "\t\t\tBUG_ON(ret != 0);",
                        "\t\t\tgoto retry;",
                        "\t\t}",
                        "\t\tif (ret != 1) {",
                        "\t\t\tBUG_ON(ret > 1);",
                        "\t\t\tif (!pages_done)",
                        "\t\t\t\tpages_done = ret;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "\t\tnr_pages--;",
                        "\t\tpages_done++;",
                        "\t\tif (!nr_pages)",
                        "\t\t\tbreak;",
                        "\t\tif (likely(pages))",
                        "\t\t\tpages++;",
                        "\t\tstart += PAGE_SIZE;",
                        "\t}",
                        "\tif (lock_dropped && *locked) {",
                        "\t\t/*",
                        "\t\t * We must let the caller know we temporarily dropped the lock",
                        "\t\t * and so the critical section protected by it was lost.",
                        "\t\t */",
                        "\t\tmmap_read_unlock(mm);",
                        "\t\t*locked = 0;",
                        "\t}",
                        "\treturn pages_done;",
                        "}"
                    ],
                    "start": 1312,
                    "highlight": 1352
                },
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n1745",
                    "code": [
                        "/*",
                        " * __gup_longterm_locked() is a wrapper for __get_user_pages_locked which",
                        " * allows us to process the FOLL_LONGTERM flag.",
                        " */",
                        "static long __gup_longterm_locked(struct mm_struct *mm,",
                        "\t\t\t\t  unsigned long start,",
                        "\t\t\t\t  unsigned long nr_pages,",
                        "\t\t\t\t  struct page **pages,",
                        "\t\t\t\t  struct vm_area_struct **vmas,",
                        "\t\t\t\t  unsigned int gup_flags)",
                        "{",
                        "\tunsigned int flags;",
                        "\tlong rc;",
                        "",
                        "\tif (!(gup_flags & FOLL_LONGTERM))",
                        "\t\treturn __get_user_pages_locked(mm, start, nr_pages, pages, vmas,",
                        "\t\t\t\t\t       NULL, gup_flags);",
                        "\tflags = memalloc_pin_save();",
                        "\tdo {",
                        "\t\trc = __get_user_pages_locked(mm, start, nr_pages, pages, vmas,",
                        "\t\t\t\t\t     NULL, gup_flags);",
                        "\t\tif (rc <= 0)",
                        "\t\t\tbreak;",
                        "\t\trc = check_and_migrate_movable_pages(rc, pages, gup_flags);",
                        "\t} while (!rc);",
                        "\tmemalloc_pin_restore(flags);",
                        "",
                        "\treturn rc;",
                        "}"
                    ],
                    "start": 1726,
                    "highlight": 1745
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff811a328d"
        },
        "3857": {
            "name": "pin_user_pages",
            "parent_idx": 3834,
            "source_line": [
                {
                    "file": "fs/io_uring.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=3dbdb38e#n8381",
                    "code": [
                        "static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,",
                        "\t\t\t\t  struct io_mapped_ubuf **pimu,",
                        "\t\t\t\t  struct page **last_hpage)",
                        "{",
                        "\tstruct io_mapped_ubuf *imu = NULL;",
                        "\tstruct vm_area_struct **vmas = NULL;",
                        "\tstruct page **pages = NULL;",
                        "\tunsigned long off, start, end, ubuf;",
                        "\tsize_t size;",
                        "\tint ret, pret, nr_pages, i;",
                        "",
                        "\tif (!iov->iov_base) {",
                        "\t\t*pimu = ctx->dummy_ubuf;",
                        "\t\treturn 0;",
                        "\t}",
                        "",
                        "\tubuf = (unsigned long) iov->iov_base;",
                        "\tend = (ubuf + iov->iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;",
                        "\tstart = ubuf >> PAGE_SHIFT;",
                        "\tnr_pages = end - start;",
                        "",
                        "\t*pimu = NULL;",
                        "\tret = -ENOMEM;",
                        "",
                        "\tpages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);",
                        "\tif (!pages)",
                        "\t\tgoto done;",
                        "",
                        "\tvmas = kvmalloc_array(nr_pages, sizeof(struct vm_area_struct *),",
                        "\t\t\t      GFP_KERNEL);",
                        "\tif (!vmas)",
                        "\t\tgoto done;",
                        "",
                        "\timu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);",
                        "\tif (!imu)",
                        "\t\tgoto done;",
                        "",
                        "\tret = 0;",
                        "\tmmap_read_lock(current->mm);",
                        "\tpret = pin_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,",
                        "\t\t\t      pages, vmas);",
                        "\tif (pret == nr_pages) {",
                        "\t\t/* don't support file backed memory */",
                        "\t\tfor (i = 0; i < nr_pages; i++) {",
                        "\t\t\tstruct vm_area_struct *vma = vmas[i];",
                        "",
                        "\t\t\tif (vma_is_shmem(vma))",
                        "\t\t\t\tcontinue;",
                        "\t\t\tif (vma->vm_file &&",
                        "\t\t\t    !is_file_hugepages(vma->vm_file)) {",
                        "\t\t\t\tret = -EOPNOTSUPP;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t}",
                        "\t} else {",
                        "\t\tret = pret < 0 ? pret : -EFAULT;",
                        "\t}",
                        "\tmmap_read_unlock(current->mm);",
                        "\tif (ret) {",
                        "\t\t/*",
                        "\t\t * if we did partial map, or found file backed vmas,",
                        "\t\t * release any pages we did get",
                        "\t\t */",
                        "\t\tif (pret > 0)",
                        "\t\t\tunpin_user_pages(pages, pret);",
                        "\t\tgoto done;",
                        "\t}",
                        "",
                        "\tret = io_buffer_account_pin(ctx, pages, pret, imu, last_hpage);",
                        "\tif (ret) {",
                        "\t\tunpin_user_pages(pages, pret);",
                        "\t\tgoto done;",
                        "\t}",
                        "",
                        "\toff = ubuf & ~PAGE_MASK;",
                        "\tsize = iov->iov_len;",
                        "\tfor (i = 0; i < nr_pages; i++) {",
                        "\t\tsize_t vec_len;",
                        "",
                        "\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);",
                        "\t\timu->bvec[i].bv_page = pages[i];",
                        "\t\timu->bvec[i].bv_len = vec_len;",
                        "\t\timu->bvec[i].bv_offset = off;",
                        "\t\toff = 0;",
                        "\t\tsize -= vec_len;",
                        "\t}",
                        "\t/* store original address for later verification */",
                        "\timu->ubuf = ubuf;",
                        "\timu->ubuf_end = ubuf + iov->iov_len;",
                        "\timu->nr_bvecs = nr_pages;",
                        "\t*pimu = imu;",
                        "\tret = 0;",
                        "done:",
                        "\tif (ret)",
                        "\t\tkvfree(imu);",
                        "\tkvfree(pages);",
                        "\tkvfree(vmas);",
                        "\treturn ret;",
                        "}"
                    ],
                    "start": 8342,
                    "highlight": 8381
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff81248733"
        },
        "3834": {
            "name": "io_sqe_buffer_register",
            "parent_idx": 3801,
            "source_line": [
                {
                    "file": "fs/io_uring.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=3dbdb38e#n8508",
                    "code": [
                        "static int io_sqe_buffers_register(struct io_ring_ctx *ctx, void __user *arg,",
                        "\t\t\t\t   unsigned int nr_args, u64 __user *tags)",
                        "{",
                        "\tstruct page *last_hpage = NULL;",
                        "\tstruct io_rsrc_data *data;",
                        "\tint i, ret;",
                        "\tstruct iovec iov;",
                        "",
                        "\tif (ctx->user_bufs)",
                        "\t\treturn -EBUSY;",
                        "\tif (!nr_args || nr_args > IORING_MAX_REG_BUFFERS)",
                        "\t\treturn -EINVAL;",
                        "\tret = io_rsrc_node_switch_start(ctx);",
                        "\tif (ret)",
                        "\t\treturn ret;",
                        "\tret = io_rsrc_data_alloc(ctx, io_rsrc_buf_put, tags, nr_args, &data);",
                        "\tif (ret)",
                        "\t\treturn ret;",
                        "\tret = io_buffers_map_alloc(ctx, nr_args);",
                        "\tif (ret) {",
                        "\t\tio_rsrc_data_free(data);",
                        "\t\treturn ret;",
                        "\t}",
                        "",
                        "\tfor (i = 0; i < nr_args; i++, ctx->nr_user_bufs++) {",
                        "\t\tret = io_copy_iov(ctx, &iov, arg, i);",
                        "\t\tif (ret)",
                        "\t\t\tbreak;",
                        "\t\tret = io_buffer_validate(&iov);",
                        "\t\tif (ret)",
                        "\t\t\tbreak;",
                        "\t\tif (!iov.iov_base && *io_get_tag_slot(data, i)) {",
                        "\t\t\tret = -EINVAL;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\tret = io_sqe_buffer_register(ctx, &iov, &ctx->user_bufs[i],",
                        "\t\t\t\t\t     &last_hpage);",
                        "\t\tif (ret)",
                        "\t\t\tbreak;",
                        "\t}",
                        "",
                        "\tWARN_ON_ONCE(ctx->buf_data);",
                        "",
                        "\tctx->buf_data = data;",
                        "\tif (ret)",
                        "\t\t__io_sqe_buffers_unregister(ctx);",
                        "\telse",
                        "\t\tio_rsrc_node_switch(ctx, NULL);",
                        "\treturn ret;",
                        "}"
                    ],
                    "start": 8472,
                    "highlight": 8508
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff8124ab24"
        },
        "3801": {
            "name": "io_sqe_buffers_register",
            "parent_idx": 3797,
            "source_line": [
                {
                    "file": "fs/io_uring.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=3dbdb38e#n10129",
                    "code": [
                        "static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,",
                        "\t\t\t       void __user *arg, unsigned nr_args)",
                        "\t__releases(ctx->uring_lock)",
                        "\t__acquires(ctx->uring_lock)",
                        "{",
                        "\tint ret;",
                        "",
                        "\t/*",
                        "\t * We're inside the ring mutex, if the ref is already dying, then",
                        "\t * someone else killed the ctx or is already going through",
                        "\t * io_uring_register().",
                        "\t */",
                        "\tif (percpu_ref_is_dying(&ctx->refs))",
                        "\t\treturn -ENXIO;",
                        "",
                        "\tif (ctx->restricted) {",
                        "\t\tif (opcode >= IORING_REGISTER_LAST)",
                        "\t\t\treturn -EINVAL;",
                        "\t\topcode = array_index_nospec(opcode, IORING_REGISTER_LAST);",
                        "\t\tif (!test_bit(opcode, ctx->restrictions.register_op))",
                        "\t\t\treturn -EACCES;",
                        "\t}",
                        "",
                        "\tif (io_register_op_must_quiesce(opcode)) {",
                        "\t\tpercpu_ref_kill(&ctx->refs);",
                        "",
                        "\t\t/*",
                        "\t\t * Drop uring mutex before waiting for references to exit. If",
                        "\t\t * another thread is currently inside io_uring_enter() it might",
                        "\t\t * need to grab the uring_lock to make progress. If we hold it",
                        "\t\t * here across the drain wait, then we can deadlock. It's safe",
                        "\t\t * to drop the mutex here, since no new references will come in",
                        "\t\t * after we've killed the percpu ref.",
                        "\t\t */",
                        "\t\tmutex_unlock(&ctx->uring_lock);",
                        "\t\tdo {",
                        "\t\t\tret = wait_for_completion_interruptible(&ctx->ref_comp);",
                        "\t\t\tif (!ret)",
                        "\t\t\t\tbreak;",
                        "\t\t\tret = io_run_task_work_sig();",
                        "\t\t\tif (ret < 0)",
                        "\t\t\t\tbreak;",
                        "\t\t} while (1);",
                        "\t\tmutex_lock(&ctx->uring_lock);",
                        "",
                        "\t\tif (ret) {",
                        "\t\t\tio_refs_resurrect(&ctx->refs, &ctx->ref_comp);",
                        "\t\t\treturn ret;",
                        "\t\t}",
                        "\t}",
                        "",
                        "\tswitch (opcode) {",
                        "\tcase IORING_REGISTER_BUFFERS:",
                        "\t\tret = io_sqe_buffers_register(ctx, arg, nr_args, NULL);",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_BUFFERS:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_sqe_buffers_unregister(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_FILES:",
                        "\t\tret = io_sqe_files_register(ctx, arg, nr_args, NULL);",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_FILES:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_sqe_files_unregister(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_FILES_UPDATE:",
                        "\t\tret = io_register_files_update(ctx, arg, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_EVENTFD:",
                        "\tcase IORING_REGISTER_EVENTFD_ASYNC:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (nr_args != 1)",
                        "\t\t\tbreak;",
                        "\t\tret = io_eventfd_register(ctx, arg);",
                        "\t\tif (ret)",
                        "\t\t\tbreak;",
                        "\t\tif (opcode == IORING_REGISTER_EVENTFD_ASYNC)",
                        "\t\t\tctx->eventfd_async = 1;",
                        "\t\telse",
                        "\t\t\tctx->eventfd_async = 0;",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_EVENTFD:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_eventfd_unregister(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_PROBE:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (!arg || nr_args > 256)",
                        "\t\t\tbreak;",
                        "\t\tret = io_probe(ctx, arg, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_PERSONALITY:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_register_personality(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_PERSONALITY:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg)",
                        "\t\t\tbreak;",
                        "\t\tret = io_unregister_personality(ctx, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_ENABLE_RINGS:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_register_enable_rings(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_RESTRICTIONS:",
                        "\t\tret = io_register_restrictions(ctx, arg, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_FILES2:",
                        "\t\tret = io_register_rsrc(ctx, arg, nr_args, IORING_RSRC_FILE);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_FILES_UPDATE2:",
                        "\t\tret = io_register_rsrc_update(ctx, arg, nr_args,",
                        "\t\t\t\t\t      IORING_RSRC_FILE);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_BUFFERS2:",
                        "\t\tret = io_register_rsrc(ctx, arg, nr_args, IORING_RSRC_BUFFER);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_BUFFERS_UPDATE:",
                        "\t\tret = io_register_rsrc_update(ctx, arg, nr_args,",
                        "\t\t\t\t\t      IORING_RSRC_BUFFER);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_IOWQ_AFF:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (!arg || !nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_register_iowq_aff(ctx, arg, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_IOWQ_AFF:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_unregister_iowq_aff(ctx);",
                        "\t\tbreak;",
                        "\tdefault:",
                        "\t\tret = -EINVAL;",
                        "\t\tbreak;",
                        "\t}",
                        "",
                        "\tif (io_register_op_must_quiesce(opcode)) {",
                        "\t\t/* bring the ctx back to life */",
                        "\t\tpercpu_ref_reinit(&ctx->refs);",
                        "\t\treinit_completion(&ctx->ref_comp);",
                        "\t}",
                        "\treturn ret;",
                        "}"
                    ],
                    "start": 10076,
                    "highlight": 10129
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff8124fc60"
        },
        "3797": {
            "name": "__io_uring_register",
            "parent_idx": 3794,
            "source_line": [
                {
                    "file": "fs/io_uring.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=3dbdb38e#n10254",
                    "code": [
                        "SYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,",
                        "\t\tvoid __user *, arg, unsigned int, nr_args)",
                        "{",
                        "\tstruct io_ring_ctx *ctx;",
                        "\tlong ret = -EBADF;",
                        "\tstruct fd f;",
                        "",
                        "\tf = fdget(fd);",
                        "\tif (!f.file)",
                        "\t\treturn -EBADF;",
                        "",
                        "\tret = -EOPNOTSUPP;",
                        "\tif (f.file->f_op != &io_uring_fops)",
                        "\t\tgoto out_fput;",
                        "",
                        "\tctx = f.file->private_data;",
                        "",
                        "\tio_run_task_work();",
                        "",
                        "\tmutex_lock(&ctx->uring_lock);",
                        "\tret = __io_uring_register(ctx, opcode, arg, nr_args);",
                        "\tmutex_unlock(&ctx->uring_lock);",
                        "\ttrace_io_uring_register(ctx, opcode, ctx->nr_user_files, ctx->nr_user_bufs,",
                        "\t\t\t\t\t\t\tctx->cq_ev_fd != NULL, ret);",
                        "out_fput:",
                        "\tfdput(f);",
                        "\treturn ret;",
                        "}"
                    ],
                    "start": 10234,
                    "highlight": 10254
                },
                {
                    "file": "fs/io_uring.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=3dbdb38e#n10234",
                    "code": [
                        "static int __io_uring_register(struct io_ring_ctx *ctx, unsigned opcode,",
                        "\t\t\t       void __user *arg, unsigned nr_args)",
                        "\t__releases(ctx->uring_lock)",
                        "\t__acquires(ctx->uring_lock)",
                        "{",
                        "\tint ret;",
                        "",
                        "\t/*",
                        "\t * We're inside the ring mutex, if the ref is already dying, then",
                        "\t * someone else killed the ctx or is already going through",
                        "\t * io_uring_register().",
                        "\t */",
                        "\tif (percpu_ref_is_dying(&ctx->refs))",
                        "\t\treturn -ENXIO;",
                        "",
                        "\tif (ctx->restricted) {",
                        "\t\tif (opcode >= IORING_REGISTER_LAST)",
                        "\t\t\treturn -EINVAL;",
                        "\t\topcode = array_index_nospec(opcode, IORING_REGISTER_LAST);",
                        "\t\tif (!test_bit(opcode, ctx->restrictions.register_op))",
                        "\t\t\treturn -EACCES;",
                        "\t}",
                        "",
                        "\tif (io_register_op_must_quiesce(opcode)) {",
                        "\t\tpercpu_ref_kill(&ctx->refs);",
                        "",
                        "\t\t/*",
                        "\t\t * Drop uring mutex before waiting for references to exit. If",
                        "\t\t * another thread is currently inside io_uring_enter() it might",
                        "\t\t * need to grab the uring_lock to make progress. If we hold it",
                        "\t\t * here across the drain wait, then we can deadlock. It's safe",
                        "\t\t * to drop the mutex here, since no new references will come in",
                        "\t\t * after we've killed the percpu ref.",
                        "\t\t */",
                        "\t\tmutex_unlock(&ctx->uring_lock);",
                        "\t\tdo {",
                        "\t\t\tret = wait_for_completion_interruptible(&ctx->ref_comp);",
                        "\t\t\tif (!ret)",
                        "\t\t\t\tbreak;",
                        "\t\t\tret = io_run_task_work_sig();",
                        "\t\t\tif (ret < 0)",
                        "\t\t\t\tbreak;",
                        "\t\t} while (1);",
                        "\t\tmutex_lock(&ctx->uring_lock);",
                        "",
                        "\t\tif (ret) {",
                        "\t\t\tio_refs_resurrect(&ctx->refs, &ctx->ref_comp);",
                        "\t\t\treturn ret;",
                        "\t\t}",
                        "\t}",
                        "",
                        "\tswitch (opcode) {",
                        "\tcase IORING_REGISTER_BUFFERS:",
                        "\t\tret = io_sqe_buffers_register(ctx, arg, nr_args, NULL);",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_BUFFERS:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_sqe_buffers_unregister(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_FILES:",
                        "\t\tret = io_sqe_files_register(ctx, arg, nr_args, NULL);",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_FILES:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_sqe_files_unregister(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_FILES_UPDATE:",
                        "\t\tret = io_register_files_update(ctx, arg, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_EVENTFD:",
                        "\tcase IORING_REGISTER_EVENTFD_ASYNC:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (nr_args != 1)",
                        "\t\t\tbreak;",
                        "\t\tret = io_eventfd_register(ctx, arg);",
                        "\t\tif (ret)",
                        "\t\t\tbreak;",
                        "\t\tif (opcode == IORING_REGISTER_EVENTFD_ASYNC)",
                        "\t\t\tctx->eventfd_async = 1;",
                        "\t\telse",
                        "\t\t\tctx->eventfd_async = 0;",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_EVENTFD:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_eventfd_unregister(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_PROBE:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (!arg || nr_args > 256)",
                        "\t\t\tbreak;",
                        "\t\tret = io_probe(ctx, arg, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_PERSONALITY:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_register_personality(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_PERSONALITY:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg)",
                        "\t\t\tbreak;",
                        "\t\tret = io_unregister_personality(ctx, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_ENABLE_RINGS:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_register_enable_rings(ctx);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_RESTRICTIONS:",
                        "\t\tret = io_register_restrictions(ctx, arg, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_FILES2:",
                        "\t\tret = io_register_rsrc(ctx, arg, nr_args, IORING_RSRC_FILE);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_FILES_UPDATE2:",
                        "\t\tret = io_register_rsrc_update(ctx, arg, nr_args,",
                        "\t\t\t\t\t      IORING_RSRC_FILE);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_BUFFERS2:",
                        "\t\tret = io_register_rsrc(ctx, arg, nr_args, IORING_RSRC_BUFFER);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_BUFFERS_UPDATE:",
                        "\t\tret = io_register_rsrc_update(ctx, arg, nr_args,",
                        "\t\t\t\t\t      IORING_RSRC_BUFFER);",
                        "\t\tbreak;",
                        "\tcase IORING_REGISTER_IOWQ_AFF:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (!arg || !nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_register_iowq_aff(ctx, arg, nr_args);",
                        "\t\tbreak;",
                        "\tcase IORING_UNREGISTER_IOWQ_AFF:",
                        "\t\tret = -EINVAL;",
                        "\t\tif (arg || nr_args)",
                        "\t\t\tbreak;",
                        "\t\tret = io_unregister_iowq_aff(ctx);",
                        "\t\tbreak;",
                        "\tdefault:",
                        "\t\tret = -EINVAL;",
                        "\t\tbreak;",
                        "\t}",
                        "",
                        "\tif (io_register_op_must_quiesce(opcode)) {",
                        "\t\t/* bring the ctx back to life */",
                        "\t\tpercpu_ref_reinit(&ctx->refs);",
                        "\t\treinit_completion(&ctx->ref_comp);",
                        "\t}",
                        "\treturn ret;",
                        "}",
                        "",
                        "SYSCALL_DEFINE4(io_uring_register, unsigned int, fd, unsigned int, opcode,",
                        "\t\tvoid __user *, arg, unsigned int, nr_args)",
                        "{",
                        "\tstruct io_ring_ctx *ctx;",
                        "\tlong ret = -EBADF;",
                        "\tstruct fd f;",
                        "",
                        "\tf = fdget(fd);",
                        "\tif (!f.file)",
                        "\t\treturn -EBADF;",
                        "",
                        "\tret = -EOPNOTSUPP;",
                        "\tif (f.file->f_op != &io_uring_fops)",
                        "\t\tgoto out_fput;",
                        "",
                        "\tctx = f.file->private_data;",
                        "",
                        "\tio_run_task_work();",
                        "",
                        "\tmutex_lock(&ctx->uring_lock);",
                        "\tret = __io_uring_register(ctx, opcode, arg, nr_args);",
                        "\tmutex_unlock(&ctx->uring_lock);",
                        "\ttrace_io_uring_register(ctx, opcode, ctx->nr_user_files, ctx->nr_user_bufs,",
                        "\t\t\t\t\t\t\tctx->cq_ev_fd != NULL, ret);",
                        "out_fput:",
                        "\tfdput(f);",
                        "\treturn ret;",
                        "}"
                    ],
                    "start": 10076,
                    "highlight": 10234
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff812504e3"
        },
        "3794": {
            "name": "__x64_sys_io_uring_register(indirect)",
            "parent_idx": 3791,
            "source_line": [
                {
                    "file": "arch/x86/entry/common.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/x86/entry/common.c?id=3dbdb38e#n50",
                    "code": [
                        "static __always_inline bool do_syscall_x64(struct pt_regs *regs, int nr)",
                        "{",
                        "\t/*",
                        "\t * Convert negative numbers to very high and thus out of range",
                        "\t * numbers for comparisons.",
                        "\t */",
                        "\tunsigned int unr = nr;",
                        "",
                        "\tif (likely(unr < NR_syscalls)) {",
                        "\t\tunr = array_index_nospec(unr, NR_syscalls);",
                        "\t\tregs->ax = sys_call_table[unr](regs);",
                        "\t\treturn true;",
                        "\t}",
                        "\treturn false;",
                        "}"
                    ],
                    "start": 40,
                    "highlight": 50
                },
                {
                    "file": "arch/x86/entry/common.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/x86/entry/common.c?id=3dbdb38e#n80",
                    "code": [
                        "__visible noinstr void do_syscall_64(struct pt_regs *regs, int nr)",
                        "{",
                        "\tadd_random_kstack_offset();",
                        "\tnr = syscall_enter_from_user_mode(regs, nr);",
                        "",
                        "\tinstrumentation_begin();",
                        "",
                        "\tif (!do_syscall_x64(regs, nr) && !do_syscall_x32(regs, nr) && nr != -1) {",
                        "\t\t/* Invalid system call, but still a system call. */",
                        "\t\tregs->ax = __x64_sys_ni_syscall(regs);",
                        "\t}",
                        "",
                        "\tinstrumentation_end();",
                        "\tsyscall_exit_to_user_mode(regs);",
                        "}"
                    ],
                    "start": 73,
                    "highlight": 80
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff81bc1a2e"
        },
        "3791": {
            "name": "do_syscall_64",
            "parent_idx": 3784,
            "source_line": [
                {
                    "file": "arch/x86/entry/entry_64.S",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/arch/x86/entry/entry_64.S?id=3dbdb38e#n113",
                    "code": [
                        "SYM_CODE_START(entry_SYSCALL_64)",
                        "\tUNWIND_HINT_EMPTY",
                        "",
                        "\tswapgs",
                        "\t/* tss.sp2 is scratch space. */",
                        "\tmovq\t%rsp, PER_CPU_VAR(cpu_tss_rw + TSS_sp2)",
                        "\tSWITCH_TO_KERNEL_CR3 scratch_reg=%rsp",
                        "\tmovq\tPER_CPU_VAR(cpu_current_top_of_stack), %rsp",
                        "",
                        "SYM_INNER_LABEL(entry_SYSCALL_64_safe_stack, SYM_L_GLOBAL)",
                        "",
                        "\t/* Construct struct pt_regs on stack */",
                        "\tpushq\t$__USER_DS\t\t\t\t/* pt_regs->ss */",
                        "\tpushq\tPER_CPU_VAR(cpu_tss_rw + TSS_sp2)\t/* pt_regs->sp */",
                        "\tpushq\t%r11\t\t\t\t\t/* pt_regs->flags */",
                        "\tpushq\t$__USER_CS\t\t\t\t/* pt_regs->cs */",
                        "\tpushq\t%rcx\t\t\t\t\t/* pt_regs->ip */",
                        "SYM_INNER_LABEL(entry_SYSCALL_64_after_hwframe, SYM_L_GLOBAL)",
                        "\tpushq\t%rax\t\t\t\t\t/* pt_regs->orig_ax */",
                        "",
                        "\tPUSH_AND_CLEAR_REGS rax=$-ENOSYS",
                        "",
                        "\t/* IRQs are off. */",
                        "\tmovq\t%rsp, %rdi",
                        "\t/* Sign extend the lower 32bit as syscall numbers are treated as int */",
                        "\tmovslq\t%eax, %rsi",
                        "\tcall\tdo_syscall_64\t\t/* returns with IRQs disabled */",
                        "",
                        "\t/*",
                        "\t * Try to use SYSRET instead of IRET if we're returning to",
                        "\t * a completely clean 64-bit userspace context.  If we're not,",
                        "\t * go to the slow exit path.",
                        "\t * In the Xen PV case we must use iret anyway.",
                        "\t */",
                        "",
                        "\tALTERNATIVE \"\", \"jmp\tswapgs_restore_regs_and_return_to_usermode\", \\",
                        "\t\tX86_FEATURE_XENPV",
                        "",
                        "\tmovq\tRCX(%rsp), %rcx",
                        "\tmovq\tRIP(%rsp), %r11",
                        "",
                        "\tcmpq\t%rcx, %r11\t/* SYSRET requires RCX == RIP */",
                        "\tjne\tswapgs_restore_regs_and_return_to_usermode",
                        "",
                        "\t/*",
                        "\t * On Intel CPUs, SYSRET with non-canonical RCX/RIP will #GP",
                        "\t * in kernel space.  This essentially lets the user take over",
                        "\t * the kernel, since userspace controls RSP.",
                        "\t *",
                        "\t * If width of \"canonical tail\" ever becomes variable, this will need",
                        "\t * to be updated to remain correct on both old and new CPUs.",
                        "\t *",
                        "\t * Change top bits to match most significant bit (47th or 56th bit",
                        "\t * depending on paging mode) in the address.",
                        "\t */",
                        "#ifdef CONFIG_X86_5LEVEL",
                        "\tALTERNATIVE \"shl $(64 - 48), %rcx; sar $(64 - 48), %rcx\", \\",
                        "\t\t\"shl $(64 - 57), %rcx; sar $(64 - 57), %rcx\", X86_FEATURE_LA57",
                        "#else",
                        "\tshl\t$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx",
                        "\tsar\t$(64 - (__VIRTUAL_MASK_SHIFT+1)), %rcx",
                        "#endif",
                        "",
                        "\t/* If this changed %rcx, it was not canonical */",
                        "\tcmpq\t%rcx, %r11",
                        "\tjne\tswapgs_restore_regs_and_return_to_usermode",
                        "",
                        "\tcmpq\t$__USER_CS, CS(%rsp)\t\t/* CS must match SYSRET */",
                        "\tjne\tswapgs_restore_regs_and_return_to_usermode",
                        "",
                        "\tmovq\tR11(%rsp), %r11",
                        "\tcmpq\t%r11, EFLAGS(%rsp)\t\t/* R11 == RFLAGS */",
                        "\tjne\tswapgs_restore_regs_and_return_to_usermode",
                        "",
                        "\t/*",
                        "\t * SYSCALL clears RF when it saves RFLAGS in R11 and SYSRET cannot",
                        "\t * restore RF properly. If the slowpath sets it for whatever reason, we",
                        "\t * need to restore it correctly.",
                        "\t *",
                        "\t * SYSRET can restore TF, but unlike IRET, restoring TF results in a",
                        "\t * trap from userspace immediately after SYSRET.  This would cause an",
                        "\t * infinite loop whenever #DB happens with register state that satisfies",
                        "\t * the opportunistic SYSRET conditions.  For example, single-stepping",
                        "\t * this user code:",
                        "\t *",
                        "\t *           movq\t$stuck_here, %rcx",
                        "\t *           pushfq",
                        "\t *           popq %r11",
                        "\t *   stuck_here:",
                        "\t *",
                        "\t * would never get past 'stuck_here'.",
                        "\t */",
                        "\ttestq\t$(X86_EFLAGS_RF|X86_EFLAGS_TF), %r11",
                        "\tjnz\tswapgs_restore_regs_and_return_to_usermode",
                        "",
                        "\t/* nothing to check for RSP */",
                        "",
                        "\tcmpq\t$__USER_DS, SS(%rsp)\t\t/* SS must match SYSRET */",
                        "\tjne\tswapgs_restore_regs_and_return_to_usermode",
                        "",
                        "\t/*",
                        "\t * We win! This label is here just for ease of understanding",
                        "\t * perf profiles. Nothing jumps here.",
                        "\t */",
                        "syscall_return_via_sysret:",
                        "\t/* rcx and r11 are already restored (see code above) */",
                        "\tPOP_REGS pop_rdi=0 skip_r11rcx=1",
                        "",
                        "\t/*",
                        "\t * Now all regs are restored except RSP and RDI.",
                        "\t * Save old stack pointer and switch to trampoline stack.",
                        "\t */",
                        "\tmovq\t%rsp, %rdi",
                        "\tmovq\tPER_CPU_VAR(cpu_tss_rw + TSS_sp0), %rsp",
                        "\tUNWIND_HINT_EMPTY",
                        "",
                        "\tpushq\tRSP-RDI(%rdi)\t/* RSP */",
                        "\tpushq\t(%rdi)\t\t/* RDI */",
                        "",
                        "\t/*",
                        "\t * We are on the trampoline stack.  All regs except RDI are live.",
                        "\t * We can do future final exit work right here.",
                        "\t */",
                        "\tSTACKLEAK_ERASE_NOCLOBBER",
                        "",
                        "\tSWITCH_TO_USER_CR3_STACK scratch_reg=%rdi",
                        "",
                        "\tpopq\t%rdi",
                        "\tpopq\t%rsp",
                        "\tswapgs",
                        "\tsysretq",
                        "SYM_CODE_END(entry_SYSCALL_64)"
                    ],
                    "start": 87,
                    "highlight": 113
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff81c00077"
        },
        "3784": {
            "name": "339_syscall_12",
            "parent_idx": 1,
            "source_line": [],
            "ins_idx": 0,
            "addr": "0x0"
        },
        "1": {
            "name": "PoC",
            "parent_idx": 0,
            "source_line": [],
            "ins_idx": 0,
            "addr": "0x0"
        },
        "4403": {
            "name": "+0x293",
            "parent_idx": 3902,
            "source_line": [
                {
                    "file": "mm/hugetlb.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/hugetlb.c?id=3dbdb38e#n5248",
                    "code": [
                        "long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,",
                        "\t\t\t struct page **pages, struct vm_area_struct **vmas,",
                        "\t\t\t unsigned long *position, unsigned long *nr_pages,",
                        "\t\t\t long i, unsigned int flags, int *locked)",
                        "{",
                        "\tunsigned long pfn_offset;",
                        "\tunsigned long vaddr = *position;",
                        "\tunsigned long remainder = *nr_pages;",
                        "\tstruct hstate *h = hstate_vma(vma);",
                        "\tint err = -EFAULT, refs;",
                        "",
                        "\twhile (vaddr < vma->vm_end && remainder) {",
                        "\t\tpte_t *pte;",
                        "\t\tspinlock_t *ptl = NULL;",
                        "\t\tint absent;",
                        "\t\tstruct page *page;",
                        "",
                        "\t\t/*",
                        "\t\t * If we have a pending SIGKILL, don't keep faulting pages and",
                        "\t\t * potentially allocating memory.",
                        "\t\t */",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tremainder = 0;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t/*",
                        "\t\t * Some archs (sparc64, sh*) have multiple pte_ts to",
                        "\t\t * each hugepage.  We have to make sure we get the",
                        "\t\t * first, for the page indexing below to work.",
                        "\t\t *",
                        "\t\t * Note that page table lock is not held when pte is null.",
                        "\t\t */",
                        "\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h),",
                        "\t\t\t\t      huge_page_size(h));",
                        "\t\tif (pte)",
                        "\t\t\tptl = huge_pte_lock(h, mm, pte);",
                        "\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));",
                        "",
                        "\t\t/*",
                        "\t\t * When coredumping, it suits get_dump_page if we just return",
                        "\t\t * an error where there's an empty slot with no huge pagecache",
                        "\t\t * to back it.  This way, we avoid allocating a hugepage, and",
                        "\t\t * the sparse dumpfile avoids allocating disk blocks, but its",
                        "\t\t * huge holes still show up with zeroes where they need to be.",
                        "\t\t */",
                        "\t\tif (absent && (flags & FOLL_DUMP) &&",
                        "\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {",
                        "\t\t\tif (pte)",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\tremainder = 0;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t/*",
                        "\t\t * We need call hugetlb_fault for both hugepages under migration",
                        "\t\t * (in which case hugetlb_fault waits for the migration,) and",
                        "\t\t * hwpoisoned hugepages (in which case we need to prevent the",
                        "\t\t * caller from accessing to them.) In order to do this, we use",
                        "\t\t * here is_swap_pte instead of is_hugetlb_entry_migration and",
                        "\t\t * is_hugetlb_entry_hwpoisoned. This is because it simply covers",
                        "\t\t * both cases, and because we can't follow correct pages",
                        "\t\t * directly from any kind of swap entries.",
                        "\t\t */",
                        "\t\tif (absent || is_swap_pte(huge_ptep_get(pte)) ||",
                        "\t\t    ((flags & FOLL_WRITE) &&",
                        "\t\t      !huge_pte_write(huge_ptep_get(pte)))) {",
                        "\t\t\tvm_fault_t ret;",
                        "\t\t\tunsigned int fault_flags = 0;",
                        "",
                        "\t\t\tif (pte)",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\tif (flags & FOLL_WRITE)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_WRITE;",
                        "\t\t\tif (locked)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |",
                        "\t\t\t\t\tFAULT_FLAG_KILLABLE;",
                        "\t\t\tif (flags & FOLL_NOWAIT)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |",
                        "\t\t\t\t\tFAULT_FLAG_RETRY_NOWAIT;",
                        "\t\t\tif (flags & FOLL_TRIED) {",
                        "\t\t\t\t/*",
                        "\t\t\t\t * Note: FAULT_FLAG_ALLOW_RETRY and",
                        "\t\t\t\t * FAULT_FLAG_TRIED can co-exist",
                        "\t\t\t\t */",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_TRIED;",
                        "\t\t\t}",
                        "\t\t\tret = hugetlb_fault(mm, vma, vaddr, fault_flags);",
                        "\t\t\tif (ret & VM_FAULT_ERROR) {",
                        "\t\t\t\terr = vm_fault_to_errno(ret, flags);",
                        "\t\t\t\tremainder = 0;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t\tif (ret & VM_FAULT_RETRY) {",
                        "\t\t\t\tif (locked &&",
                        "\t\t\t\t    !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))",
                        "\t\t\t\t\t*locked = 0;",
                        "\t\t\t\t*nr_pages = 0;",
                        "\t\t\t\t/*",
                        "\t\t\t\t * VM_FAULT_RETRY must not return an",
                        "\t\t\t\t * error, it will return zero",
                        "\t\t\t\t * instead.",
                        "\t\t\t\t *",
                        "\t\t\t\t * No need to update \"position\" as the",
                        "\t\t\t\t * caller will not check it after",
                        "\t\t\t\t * *nr_pages is set to 0.",
                        "\t\t\t\t */",
                        "\t\t\t\treturn i;",
                        "\t\t\t}",
                        "\t\t\tcontinue;",
                        "\t\t}",
                        "",
                        "\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;",
                        "\t\tpage = pte_page(huge_ptep_get(pte));",
                        "",
                        "\t\t/*",
                        "\t\t * If subpage information not requested, update counters",
                        "\t\t * and skip the same_page loop below.",
                        "\t\t */",
                        "\t\tif (!pages && !vmas && !pfn_offset &&",
                        "\t\t    (vaddr + huge_page_size(h) < vma->vm_end) &&",
                        "\t\t    (remainder >= pages_per_huge_page(h))) {",
                        "\t\t\tvaddr += huge_page_size(h);",
                        "\t\t\tremainder -= pages_per_huge_page(h);",
                        "\t\t\ti += pages_per_huge_page(h);",
                        "\t\t\tspin_unlock(ptl);",
                        "\t\t\tcontinue;",
                        "\t\t}",
                        "",
                        "\t\trefs = min3(pages_per_huge_page(h) - pfn_offset,",
                        "\t\t\t    (vma->vm_end - vaddr) >> PAGE_SHIFT, remainder);",
                        "",
                        "\t\tif (pages || vmas)",
                        "\t\t\trecord_subpages_vmas(mem_map_offset(page, pfn_offset),",
                        "\t\t\t\t\t     vma, refs,",
                        "\t\t\t\t\t     likely(pages) ? pages + i : NULL,",
                        "\t\t\t\t\t     vmas ? vmas + i : NULL);",
                        "",
                        "\t\tif (pages) {",
                        "\t\t\t/*",
                        "\t\t\t * try_grab_compound_head() should always succeed here,",
                        "\t\t\t * because: a) we hold the ptl lock, and b) we've just",
                        "\t\t\t * checked that the huge page is present in the page",
                        "\t\t\t * tables. If the huge page is present, then the tail",
                        "\t\t\t * pages must also be present. The ptl prevents the",
                        "\t\t\t * head page and tail pages from being rearranged in",
                        "\t\t\t * any way. So this page must be available at this",
                        "\t\t\t * point, unless the page refcount overflowed:",
                        "\t\t\t */",
                        "\t\t\tif (WARN_ON_ONCE(!try_grab_compound_head(pages[i],",
                        "\t\t\t\t\t\t\t\t refs,",
                        "\t\t\t\t\t\t\t\t flags))) {",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\t\tremainder = 0;",
                        "\t\t\t\terr = -ENOMEM;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t}",
                        "",
                        "\t\tvaddr += (refs << PAGE_SHIFT);",
                        "\t\tremainder -= refs;",
                        "\t\ti += refs;",
                        "",
                        "\t\tspin_unlock(ptl);",
                        "\t}",
                        "\t*nr_pages = remainder;",
                        "\t/*",
                        "\t * setting position is actually required only if remainder is",
                        "\t * not zero but it's faster not to add a \"if (remainder)\"",
                        "\t * branch.",
                        "\t */",
                        "\t*position = vaddr;",
                        "",
                        "\treturn i ? i : err;",
                        "}"
                    ],
                    "start": 5099,
                    "highlight": 5248
                }
            ],
            "ins_idx": 136,
            "addr": "0xffffffff811d8a23"
        },
        "3846": {
            "name": "+0x77",
            "parent_idx": 3845,
            "source_line": [
                {
                    "file": "mm/util.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/util.c?id=3dbdb38e#n608",
                    "code": [
                        "/**",
                        " * kvmalloc_node - attempt to allocate physically contiguous memory, but upon",
                        " * failure, fall back to non-contiguous (vmalloc) allocation.",
                        " * @size: size of the request.",
                        " * @flags: gfp mask for the allocation - must be compatible (superset) with GFP_KERNEL.",
                        " * @node: numa node to allocate from",
                        " *",
                        " * Uses kmalloc to get the memory but if the allocation fails then falls back",
                        " * to the vmalloc allocator. Use kvfree for freeing the memory.",
                        " *",
                        " * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported.",
                        " * __GFP_RETRY_MAYFAIL is supported, and it should be used only if kmalloc is",
                        " * preferable to the vmalloc fallback, due to visible performance drawbacks.",
                        " *",
                        " * Please note that any use of gfp flags outside of GFP_KERNEL is careful to not",
                        " * fall back to vmalloc.",
                        " *",
                        " * Return: pointer to the allocated memory of %NULL in case of failure",
                        " */",
                        "void *kvmalloc_node(size_t size, gfp_t flags, int node)",
                        "{",
                        "\tgfp_t kmalloc_flags = flags;",
                        "\tvoid *ret;",
                        "",
                        "\t/*",
                        "\t * vmalloc uses GFP_KERNEL for some internal allocations (e.g page tables)",
                        "\t * so the given set of flags has to be compatible.",
                        "\t */",
                        "\tif ((flags & GFP_KERNEL) != GFP_KERNEL)",
                        "\t\treturn kmalloc_node(size, flags, node);",
                        "",
                        "\t/*",
                        "\t * We want to attempt a large physically contiguous block first because",
                        "\t * it is less likely to fragment multiple larger blocks and therefore",
                        "\t * contribute to a long term fragmentation less than vmalloc fallback.",
                        "\t * However make sure that larger requests are not too disruptive - no",
                        "\t * OOM killer and no allocation failure warnings as we have a fallback.",
                        "\t */",
                        "\tif (size > PAGE_SIZE) {",
                        "\t\tkmalloc_flags |= __GFP_NOWARN;",
                        "",
                        "\t\tif (!(kmalloc_flags & __GFP_RETRY_MAYFAIL))",
                        "\t\t\tkmalloc_flags |= __GFP_NORETRY;",
                        "\t}",
                        "",
                        "\tret = kmalloc_node(size, kmalloc_flags, node);",
                        "",
                        "\t/*",
                        "\t * It doesn't really make sense to fallback to vmalloc for sub page",
                        "\t * requests",
                        "\t */",
                        "\tif (ret || size <= PAGE_SIZE)",
                        "\t\treturn ret;",
                        "",
                        "\treturn __vmalloc_node(size, 1, flags, node,",
                        "\t\t\t__builtin_return_address(0));",
                        "}",
                        "EXPORT_SYMBOL(kvmalloc_node);",
                        "",
                        "/**",
                        " * kvfree() - Free memory.",
                        " * @addr: Pointer to allocated memory.",
                        " *",
                        " * kvfree frees memory allocated by any of vmalloc(), kmalloc() or kvmalloc().",
                        " * It is slightly more efficient to use kfree() or vfree() if you are certain",
                        " * that you know which one to use.",
                        " *",
                        " * Context: Either preemptible task context or not-NMI interrupt.",
                        " */",
                        "void kvfree(const void *addr)",
                        "{",
                        "\tif (is_vmalloc_addr(addr))",
                        "\t\tvfree(addr);",
                        "\telse",
                        "\t\tkfree(addr);",
                        "}"
                    ],
                    "start": 542,
                    "highlight": 608
                }
            ],
            "ins_idx": 225,
            "addr": "0xffffffff8118fd17"
        },
        "3845": {
            "name": "kvmalloc_node",
            "parent_idx": 3834,
            "source_line": [
                {
                    "file": "./include/linux/mm.h",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/./include/linux/mm.h?id=3dbdb38e#n798",
                    "code": [
                        "extern void *kvmalloc_node(size_t size, gfp_t flags, int node);",
                        "static inline void *kvmalloc(size_t size, gfp_t flags)",
                        "{",
                        "\treturn kvmalloc_node(size, flags, NUMA_NO_NODE);",
                        "}"
                    ],
                    "start": 795,
                    "highlight": 798
                },
                {
                    "file": "./include/linux/mm.h",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/./include/linux/mm.h?id=3dbdb38e#n816",
                    "code": [
                        "static inline void *kvmalloc_array(size_t n, size_t size, gfp_t flags)",
                        "{",
                        "\tsize_t bytes;",
                        "",
                        "\tif (unlikely(check_mul_overflow(n, size, &bytes)))",
                        "\t\treturn NULL;",
                        "",
                        "\treturn kvmalloc(bytes, flags);",
                        "}"
                    ],
                    "start": 809,
                    "highlight": 816
                },
                {
                    "file": "fs/io_uring.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=3dbdb38e#n8366",
                    "code": [
                        "static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,",
                        "\t\t\t\t  struct io_mapped_ubuf **pimu,",
                        "\t\t\t\t  struct page **last_hpage)",
                        "{",
                        "\tstruct io_mapped_ubuf *imu = NULL;",
                        "\tstruct vm_area_struct **vmas = NULL;",
                        "\tstruct page **pages = NULL;",
                        "\tunsigned long off, start, end, ubuf;",
                        "\tsize_t size;",
                        "\tint ret, pret, nr_pages, i;",
                        "",
                        "\tif (!iov->iov_base) {",
                        "\t\t*pimu = ctx->dummy_ubuf;",
                        "\t\treturn 0;",
                        "\t}",
                        "",
                        "\tubuf = (unsigned long) iov->iov_base;",
                        "\tend = (ubuf + iov->iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;",
                        "\tstart = ubuf >> PAGE_SHIFT;",
                        "\tnr_pages = end - start;",
                        "",
                        "\t*pimu = NULL;",
                        "\tret = -ENOMEM;",
                        "",
                        "\tpages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);",
                        "\tif (!pages)",
                        "\t\tgoto done;",
                        "",
                        "\tvmas = kvmalloc_array(nr_pages, sizeof(struct vm_area_struct *),",
                        "\t\t\t      GFP_KERNEL);",
                        "\tif (!vmas)",
                        "\t\tgoto done;",
                        "",
                        "\timu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);",
                        "\tif (!imu)",
                        "\t\tgoto done;",
                        "",
                        "\tret = 0;",
                        "\tmmap_read_lock(current->mm);",
                        "\tpret = pin_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,",
                        "\t\t\t      pages, vmas);",
                        "\tif (pret == nr_pages) {",
                        "\t\t/* don't support file backed memory */",
                        "\t\tfor (i = 0; i < nr_pages; i++) {",
                        "\t\t\tstruct vm_area_struct *vma = vmas[i];",
                        "",
                        "\t\t\tif (vma_is_shmem(vma))",
                        "\t\t\t\tcontinue;",
                        "\t\t\tif (vma->vm_file &&",
                        "\t\t\t    !is_file_hugepages(vma->vm_file)) {",
                        "\t\t\t\tret = -EOPNOTSUPP;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t}",
                        "\t} else {",
                        "\t\tret = pret < 0 ? pret : -EFAULT;",
                        "\t}",
                        "\tmmap_read_unlock(current->mm);",
                        "\tif (ret) {",
                        "\t\t/*",
                        "\t\t * if we did partial map, or found file backed vmas,",
                        "\t\t * release any pages we did get",
                        "\t\t */",
                        "\t\tif (pret > 0)",
                        "\t\t\tunpin_user_pages(pages, pret);",
                        "\t\tgoto done;",
                        "\t}",
                        "",
                        "\tret = io_buffer_account_pin(ctx, pages, pret, imu, last_hpage);",
                        "\tif (ret) {",
                        "\t\tunpin_user_pages(pages, pret);",
                        "\t\tgoto done;",
                        "\t}",
                        "",
                        "\toff = ubuf & ~PAGE_MASK;",
                        "\tsize = iov->iov_len;",
                        "\tfor (i = 0; i < nr_pages; i++) {",
                        "\t\tsize_t vec_len;",
                        "",
                        "\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);",
                        "\t\timu->bvec[i].bv_page = pages[i];",
                        "\t\timu->bvec[i].bv_len = vec_len;",
                        "\t\timu->bvec[i].bv_offset = off;",
                        "\t\toff = 0;",
                        "\t\tsize -= vec_len;",
                        "\t}",
                        "\t/* store original address for later verification */",
                        "\timu->ubuf = ubuf;",
                        "\timu->ubuf_end = ubuf + iov->iov_len;",
                        "\timu->nr_bvecs = nr_pages;",
                        "\t*pimu = imu;",
                        "\tret = 0;",
                        "done:",
                        "\tif (ret)",
                        "\t\tkvfree(imu);",
                        "\tkvfree(pages);",
                        "\tkvfree(vmas);",
                        "\treturn ret;",
                        "}"
                    ],
                    "start": 8342,
                    "highlight": 8366
                }
            ],
            "ins_idx": 0,
            "addr": "0xffffffff81248678"
        },
        "4402": {
            "name": "+0x286",
            "parent_idx": 3902,
            "source_line": [
                {
                    "file": "mm/hugetlb.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/hugetlb.c?id=3dbdb38e#n5248",
                    "code": [
                        "long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,",
                        "\t\t\t struct page **pages, struct vm_area_struct **vmas,",
                        "\t\t\t unsigned long *position, unsigned long *nr_pages,",
                        "\t\t\t long i, unsigned int flags, int *locked)",
                        "{",
                        "\tunsigned long pfn_offset;",
                        "\tunsigned long vaddr = *position;",
                        "\tunsigned long remainder = *nr_pages;",
                        "\tstruct hstate *h = hstate_vma(vma);",
                        "\tint err = -EFAULT, refs;",
                        "",
                        "\twhile (vaddr < vma->vm_end && remainder) {",
                        "\t\tpte_t *pte;",
                        "\t\tspinlock_t *ptl = NULL;",
                        "\t\tint absent;",
                        "\t\tstruct page *page;",
                        "",
                        "\t\t/*",
                        "\t\t * If we have a pending SIGKILL, don't keep faulting pages and",
                        "\t\t * potentially allocating memory.",
                        "\t\t */",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tremainder = 0;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t/*",
                        "\t\t * Some archs (sparc64, sh*) have multiple pte_ts to",
                        "\t\t * each hugepage.  We have to make sure we get the",
                        "\t\t * first, for the page indexing below to work.",
                        "\t\t *",
                        "\t\t * Note that page table lock is not held when pte is null.",
                        "\t\t */",
                        "\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h),",
                        "\t\t\t\t      huge_page_size(h));",
                        "\t\tif (pte)",
                        "\t\t\tptl = huge_pte_lock(h, mm, pte);",
                        "\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));",
                        "",
                        "\t\t/*",
                        "\t\t * When coredumping, it suits get_dump_page if we just return",
                        "\t\t * an error where there's an empty slot with no huge pagecache",
                        "\t\t * to back it.  This way, we avoid allocating a hugepage, and",
                        "\t\t * the sparse dumpfile avoids allocating disk blocks, but its",
                        "\t\t * huge holes still show up with zeroes where they need to be.",
                        "\t\t */",
                        "\t\tif (absent && (flags & FOLL_DUMP) &&",
                        "\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {",
                        "\t\t\tif (pte)",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\tremainder = 0;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t/*",
                        "\t\t * We need call hugetlb_fault for both hugepages under migration",
                        "\t\t * (in which case hugetlb_fault waits for the migration,) and",
                        "\t\t * hwpoisoned hugepages (in which case we need to prevent the",
                        "\t\t * caller from accessing to them.) In order to do this, we use",
                        "\t\t * here is_swap_pte instead of is_hugetlb_entry_migration and",
                        "\t\t * is_hugetlb_entry_hwpoisoned. This is because it simply covers",
                        "\t\t * both cases, and because we can't follow correct pages",
                        "\t\t * directly from any kind of swap entries.",
                        "\t\t */",
                        "\t\tif (absent || is_swap_pte(huge_ptep_get(pte)) ||",
                        "\t\t    ((flags & FOLL_WRITE) &&",
                        "\t\t      !huge_pte_write(huge_ptep_get(pte)))) {",
                        "\t\t\tvm_fault_t ret;",
                        "\t\t\tunsigned int fault_flags = 0;",
                        "",
                        "\t\t\tif (pte)",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\tif (flags & FOLL_WRITE)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_WRITE;",
                        "\t\t\tif (locked)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |",
                        "\t\t\t\t\tFAULT_FLAG_KILLABLE;",
                        "\t\t\tif (flags & FOLL_NOWAIT)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |",
                        "\t\t\t\t\tFAULT_FLAG_RETRY_NOWAIT;",
                        "\t\t\tif (flags & FOLL_TRIED) {",
                        "\t\t\t\t/*",
                        "\t\t\t\t * Note: FAULT_FLAG_ALLOW_RETRY and",
                        "\t\t\t\t * FAULT_FLAG_TRIED can co-exist",
                        "\t\t\t\t */",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_TRIED;",
                        "\t\t\t}",
                        "\t\t\tret = hugetlb_fault(mm, vma, vaddr, fault_flags);",
                        "\t\t\tif (ret & VM_FAULT_ERROR) {",
                        "\t\t\t\terr = vm_fault_to_errno(ret, flags);",
                        "\t\t\t\tremainder = 0;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t\tif (ret & VM_FAULT_RETRY) {",
                        "\t\t\t\tif (locked &&",
                        "\t\t\t\t    !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))",
                        "\t\t\t\t\t*locked = 0;",
                        "\t\t\t\t*nr_pages = 0;",
                        "\t\t\t\t/*",
                        "\t\t\t\t * VM_FAULT_RETRY must not return an",
                        "\t\t\t\t * error, it will return zero",
                        "\t\t\t\t * instead.",
                        "\t\t\t\t *",
                        "\t\t\t\t * No need to update \"position\" as the",
                        "\t\t\t\t * caller will not check it after",
                        "\t\t\t\t * *nr_pages is set to 0.",
                        "\t\t\t\t */",
                        "\t\t\t\treturn i;",
                        "\t\t\t}",
                        "\t\t\tcontinue;",
                        "\t\t}",
                        "",
                        "\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;",
                        "\t\tpage = pte_page(huge_ptep_get(pte));",
                        "",
                        "\t\t/*",
                        "\t\t * If subpage information not requested, update counters",
                        "\t\t * and skip the same_page loop below.",
                        "\t\t */",
                        "\t\tif (!pages && !vmas && !pfn_offset &&",
                        "\t\t    (vaddr + huge_page_size(h) < vma->vm_end) &&",
                        "\t\t    (remainder >= pages_per_huge_page(h))) {",
                        "\t\t\tvaddr += huge_page_size(h);",
                        "\t\t\tremainder -= pages_per_huge_page(h);",
                        "\t\t\ti += pages_per_huge_page(h);",
                        "\t\t\tspin_unlock(ptl);",
                        "\t\t\tcontinue;",
                        "\t\t}",
                        "",
                        "\t\trefs = min3(pages_per_huge_page(h) - pfn_offset,",
                        "\t\t\t    (vma->vm_end - vaddr) >> PAGE_SHIFT, remainder);",
                        "",
                        "\t\tif (pages || vmas)",
                        "\t\t\trecord_subpages_vmas(mem_map_offset(page, pfn_offset),",
                        "\t\t\t\t\t     vma, refs,",
                        "\t\t\t\t\t     likely(pages) ? pages + i : NULL,",
                        "\t\t\t\t\t     vmas ? vmas + i : NULL);",
                        "",
                        "\t\tif (pages) {",
                        "\t\t\t/*",
                        "\t\t\t * try_grab_compound_head() should always succeed here,",
                        "\t\t\t * because: a) we hold the ptl lock, and b) we've just",
                        "\t\t\t * checked that the huge page is present in the page",
                        "\t\t\t * tables. If the huge page is present, then the tail",
                        "\t\t\t * pages must also be present. The ptl prevents the",
                        "\t\t\t * head page and tail pages from being rearranged in",
                        "\t\t\t * any way. So this page must be available at this",
                        "\t\t\t * point, unless the page refcount overflowed:",
                        "\t\t\t */",
                        "\t\t\tif (WARN_ON_ONCE(!try_grab_compound_head(pages[i],",
                        "\t\t\t\t\t\t\t\t refs,",
                        "\t\t\t\t\t\t\t\t flags))) {",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\t\tremainder = 0;",
                        "\t\t\t\terr = -ENOMEM;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t}",
                        "",
                        "\t\tvaddr += (refs << PAGE_SHIFT);",
                        "\t\tremainder -= refs;",
                        "\t\ti += refs;",
                        "",
                        "\t\tspin_unlock(ptl);",
                        "\t}",
                        "\t*nr_pages = remainder;",
                        "\t/*",
                        "\t * setting position is actually required only if remainder is",
                        "\t * not zero but it's faster not to add a \"if (remainder)\"",
                        "\t * branch.",
                        "\t */",
                        "\t*position = vaddr;",
                        "",
                        "\treturn i ? i : err;",
                        "}"
                    ],
                    "start": 5099,
                    "highlight": 5248
                }
            ],
            "ins_idx": 224,
            "addr": "0xffffffff811d8a16"
        },
        "3906": {
            "name": "+0x18",
            "parent_idx": 3902,
            "source_line": [
                {
                    "file": "mm/hugetlb.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/hugetlb.c?id=3dbdb38e#n5103",
                    "code": [
                        "long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,",
                        "\t\t\t struct page **pages, struct vm_area_struct **vmas,",
                        "\t\t\t unsigned long *position, unsigned long *nr_pages,",
                        "\t\t\t long i, unsigned int flags, int *locked)",
                        "{",
                        "\tunsigned long pfn_offset;",
                        "\tunsigned long vaddr = *position;",
                        "\tunsigned long remainder = *nr_pages;",
                        "\tstruct hstate *h = hstate_vma(vma);",
                        "\tint err = -EFAULT, refs;",
                        "",
                        "\twhile (vaddr < vma->vm_end && remainder) {",
                        "\t\tpte_t *pte;",
                        "\t\tspinlock_t *ptl = NULL;",
                        "\t\tint absent;",
                        "\t\tstruct page *page;",
                        "",
                        "\t\t/*",
                        "\t\t * If we have a pending SIGKILL, don't keep faulting pages and",
                        "\t\t * potentially allocating memory.",
                        "\t\t */",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tremainder = 0;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t/*",
                        "\t\t * Some archs (sparc64, sh*) have multiple pte_ts to",
                        "\t\t * each hugepage.  We have to make sure we get the",
                        "\t\t * first, for the page indexing below to work.",
                        "\t\t *",
                        "\t\t * Note that page table lock is not held when pte is null.",
                        "\t\t */",
                        "\t\tpte = huge_pte_offset(mm, vaddr & huge_page_mask(h),",
                        "\t\t\t\t      huge_page_size(h));",
                        "\t\tif (pte)",
                        "\t\t\tptl = huge_pte_lock(h, mm, pte);",
                        "\t\tabsent = !pte || huge_pte_none(huge_ptep_get(pte));",
                        "",
                        "\t\t/*",
                        "\t\t * When coredumping, it suits get_dump_page if we just return",
                        "\t\t * an error where there's an empty slot with no huge pagecache",
                        "\t\t * to back it.  This way, we avoid allocating a hugepage, and",
                        "\t\t * the sparse dumpfile avoids allocating disk blocks, but its",
                        "\t\t * huge holes still show up with zeroes where they need to be.",
                        "\t\t */",
                        "\t\tif (absent && (flags & FOLL_DUMP) &&",
                        "\t\t    !hugetlbfs_pagecache_present(h, vma, vaddr)) {",
                        "\t\t\tif (pte)",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\tremainder = 0;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t/*",
                        "\t\t * We need call hugetlb_fault for both hugepages under migration",
                        "\t\t * (in which case hugetlb_fault waits for the migration,) and",
                        "\t\t * hwpoisoned hugepages (in which case we need to prevent the",
                        "\t\t * caller from accessing to them.) In order to do this, we use",
                        "\t\t * here is_swap_pte instead of is_hugetlb_entry_migration and",
                        "\t\t * is_hugetlb_entry_hwpoisoned. This is because it simply covers",
                        "\t\t * both cases, and because we can't follow correct pages",
                        "\t\t * directly from any kind of swap entries.",
                        "\t\t */",
                        "\t\tif (absent || is_swap_pte(huge_ptep_get(pte)) ||",
                        "\t\t    ((flags & FOLL_WRITE) &&",
                        "\t\t      !huge_pte_write(huge_ptep_get(pte)))) {",
                        "\t\t\tvm_fault_t ret;",
                        "\t\t\tunsigned int fault_flags = 0;",
                        "",
                        "\t\t\tif (pte)",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\tif (flags & FOLL_WRITE)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_WRITE;",
                        "\t\t\tif (locked)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |",
                        "\t\t\t\t\tFAULT_FLAG_KILLABLE;",
                        "\t\t\tif (flags & FOLL_NOWAIT)",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_ALLOW_RETRY |",
                        "\t\t\t\t\tFAULT_FLAG_RETRY_NOWAIT;",
                        "\t\t\tif (flags & FOLL_TRIED) {",
                        "\t\t\t\t/*",
                        "\t\t\t\t * Note: FAULT_FLAG_ALLOW_RETRY and",
                        "\t\t\t\t * FAULT_FLAG_TRIED can co-exist",
                        "\t\t\t\t */",
                        "\t\t\t\tfault_flags |= FAULT_FLAG_TRIED;",
                        "\t\t\t}",
                        "\t\t\tret = hugetlb_fault(mm, vma, vaddr, fault_flags);",
                        "\t\t\tif (ret & VM_FAULT_ERROR) {",
                        "\t\t\t\terr = vm_fault_to_errno(ret, flags);",
                        "\t\t\t\tremainder = 0;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t\tif (ret & VM_FAULT_RETRY) {",
                        "\t\t\t\tif (locked &&",
                        "\t\t\t\t    !(fault_flags & FAULT_FLAG_RETRY_NOWAIT))",
                        "\t\t\t\t\t*locked = 0;",
                        "\t\t\t\t*nr_pages = 0;",
                        "\t\t\t\t/*",
                        "\t\t\t\t * VM_FAULT_RETRY must not return an",
                        "\t\t\t\t * error, it will return zero",
                        "\t\t\t\t * instead.",
                        "\t\t\t\t *",
                        "\t\t\t\t * No need to update \"position\" as the",
                        "\t\t\t\t * caller will not check it after",
                        "\t\t\t\t * *nr_pages is set to 0.",
                        "\t\t\t\t */",
                        "\t\t\t\treturn i;",
                        "\t\t\t}",
                        "\t\t\tcontinue;",
                        "\t\t}",
                        "",
                        "\t\tpfn_offset = (vaddr & ~huge_page_mask(h)) >> PAGE_SHIFT;",
                        "\t\tpage = pte_page(huge_ptep_get(pte));",
                        "",
                        "\t\t/*",
                        "\t\t * If subpage information not requested, update counters",
                        "\t\t * and skip the same_page loop below.",
                        "\t\t */",
                        "\t\tif (!pages && !vmas && !pfn_offset &&",
                        "\t\t    (vaddr + huge_page_size(h) < vma->vm_end) &&",
                        "\t\t    (remainder >= pages_per_huge_page(h))) {",
                        "\t\t\tvaddr += huge_page_size(h);",
                        "\t\t\tremainder -= pages_per_huge_page(h);",
                        "\t\t\ti += pages_per_huge_page(h);",
                        "\t\t\tspin_unlock(ptl);",
                        "\t\t\tcontinue;",
                        "\t\t}",
                        "",
                        "\t\trefs = min3(pages_per_huge_page(h) - pfn_offset,",
                        "\t\t\t    (vma->vm_end - vaddr) >> PAGE_SHIFT, remainder);",
                        "",
                        "\t\tif (pages || vmas)",
                        "\t\t\trecord_subpages_vmas(mem_map_offset(page, pfn_offset),",
                        "\t\t\t\t\t     vma, refs,",
                        "\t\t\t\t\t     likely(pages) ? pages + i : NULL,",
                        "\t\t\t\t\t     vmas ? vmas + i : NULL);",
                        "",
                        "\t\tif (pages) {",
                        "\t\t\t/*",
                        "\t\t\t * try_grab_compound_head() should always succeed here,",
                        "\t\t\t * because: a) we hold the ptl lock, and b) we've just",
                        "\t\t\t * checked that the huge page is present in the page",
                        "\t\t\t * tables. If the huge page is present, then the tail",
                        "\t\t\t * pages must also be present. The ptl prevents the",
                        "\t\t\t * head page and tail pages from being rearranged in",
                        "\t\t\t * any way. So this page must be available at this",
                        "\t\t\t * point, unless the page refcount overflowed:",
                        "\t\t\t */",
                        "\t\t\tif (WARN_ON_ONCE(!try_grab_compound_head(pages[i],",
                        "\t\t\t\t\t\t\t\t refs,",
                        "\t\t\t\t\t\t\t\t flags))) {",
                        "\t\t\t\tspin_unlock(ptl);",
                        "\t\t\t\tremainder = 0;",
                        "\t\t\t\terr = -ENOMEM;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t}",
                        "",
                        "\t\tvaddr += (refs << PAGE_SHIFT);",
                        "\t\tremainder -= refs;",
                        "\t\ti += refs;",
                        "",
                        "\t\tspin_unlock(ptl);",
                        "\t}",
                        "\t*nr_pages = remainder;",
                        "\t/*",
                        "\t * setting position is actually required only if remainder is",
                        "\t * not zero but it's faster not to add a \"if (remainder)\"",
                        "\t * branch.",
                        "\t */",
                        "\t*position = vaddr;",
                        "",
                        "\treturn i ? i : err;",
                        "}"
                    ],
                    "start": 5099,
                    "highlight": 5103
                }
            ],
            "ins_idx": 323,
            "addr": "0xffffffff811d87a8"
        },
        "3899": {
            "name": "+0x12f",
            "parent_idx": 3868,
            "source_line": [
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n1137",
                    "code": [
                        "/**",
                        " * __get_user_pages() - pin user pages in memory",
                        " * @mm:\t\tmm_struct of target mm",
                        " * @start:\tstarting user address",
                        " * @nr_pages:\tnumber of pages from start to pin",
                        " * @gup_flags:\tflags modifying pin behaviour",
                        " * @pages:\tarray that receives pointers to the pages pinned.",
                        " *\t\tShould be at least nr_pages long. Or NULL, if caller",
                        " *\t\tonly intends to ensure the pages are faulted in.",
                        " * @vmas:\tarray of pointers to vmas corresponding to each page.",
                        " *\t\tOr NULL if the caller does not require them.",
                        " * @locked:     whether we're still with the mmap_lock held",
                        " *",
                        " * Returns either number of pages pinned (which may be less than the",
                        " * number requested), or an error. Details about the return value:",
                        " *",
                        " * -- If nr_pages is 0, returns 0.",
                        " * -- If nr_pages is >0, but no pages were pinned, returns -errno.",
                        " * -- If nr_pages is >0, and some pages were pinned, returns the number of",
                        " *    pages pinned. Again, this may be less than nr_pages.",
                        " * -- 0 return value is possible when the fault would need to be retried.",
                        " *",
                        " * The caller is responsible for releasing returned @pages, via put_page().",
                        " *",
                        " * @vmas are valid only as long as mmap_lock is held.",
                        " *",
                        " * Must be called with mmap_lock held.  It may be released.  See below.",
                        " *",
                        " * __get_user_pages walks a process's page tables and takes a reference to",
                        " * each struct page that each user address corresponds to at a given",
                        " * instant. That is, it takes the page that would be accessed if a user",
                        " * thread accesses the given user virtual address at that instant.",
                        " *",
                        " * This does not guarantee that the page exists in the user mappings when",
                        " * __get_user_pages returns, and there may even be a completely different",
                        " * page there in some cases (eg. if mmapped pagecache has been invalidated",
                        " * and subsequently re faulted). However it does guarantee that the page",
                        " * won't be freed completely. And mostly callers simply care that the page",
                        " * contains data that was valid *at some point in time*. Typically, an IO",
                        " * or similar operation cannot guarantee anything stronger anyway because",
                        " * locks can't be held over the syscall boundary.",
                        " *",
                        " * If @gup_flags & FOLL_WRITE == 0, the page must not be written to. If",
                        " * the page is written to, set_page_dirty (or set_page_dirty_lock, as",
                        " * appropriate) must be called after the page is finished with, and",
                        " * before put_page is called.",
                        " *",
                        " * If @locked != NULL, *@locked will be set to 0 when mmap_lock is",
                        " * released by an up_read().  That can happen if @gup_flags does not",
                        " * have FOLL_NOWAIT.",
                        " *",
                        " * A caller using such a combination of @locked and @gup_flags",
                        " * must therefore hold the mmap_lock for reading only, and recognize",
                        " * when it's been released.  Otherwise, it must be held for either",
                        " * reading or writing and will not be released.",
                        " *",
                        " * In most cases, get_user_pages or get_user_pages_fast should be used",
                        " * instead of __get_user_pages. __get_user_pages should be used only if",
                        " * you need some special @gup_flags.",
                        " */",
                        "static long __get_user_pages(struct mm_struct *mm,",
                        "\t\tunsigned long start, unsigned long nr_pages,",
                        "\t\tunsigned int gup_flags, struct page **pages,",
                        "\t\tstruct vm_area_struct **vmas, int *locked)",
                        "{",
                        "\tlong ret = 0, i = 0;",
                        "\tstruct vm_area_struct *vma = NULL;",
                        "\tstruct follow_page_context ctx = { NULL };",
                        "",
                        "\tif (!nr_pages)",
                        "\t\treturn 0;",
                        "",
                        "\tstart = untagged_addr(start);",
                        "",
                        "\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));",
                        "",
                        "\t/*",
                        "\t * If FOLL_FORCE is set then do not force a full fault as the hinting",
                        "\t * fault information is unrelated to the reference behaviour of a task",
                        "\t * using the address space",
                        "\t */",
                        "\tif (!(gup_flags & FOLL_FORCE))",
                        "\t\tgup_flags |= FOLL_NUMA;",
                        "",
                        "\tdo {",
                        "\t\tstruct page *page;",
                        "\t\tunsigned int foll_flags = gup_flags;",
                        "\t\tunsigned int page_increm;",
                        "",
                        "\t\t/* first iteration or cross vma bound */",
                        "\t\tif (!vma || start >= vma->vm_end) {",
                        "\t\t\tvma = find_extend_vma(mm, start);",
                        "\t\t\tif (!vma && in_gate_area(mm, start)) {",
                        "\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,",
                        "\t\t\t\t\t\tgup_flags, &vma,",
                        "\t\t\t\t\t\tpages ? &pages[i] : NULL);",
                        "\t\t\t\tif (ret)",
                        "\t\t\t\t\tgoto out;",
                        "\t\t\t\tctx.page_mask = 0;",
                        "\t\t\t\tgoto next_page;",
                        "\t\t\t}",
                        "",
                        "\t\t\tif (!vma) {",
                        "\t\t\t\tret = -EFAULT;",
                        "\t\t\t\tgoto out;",
                        "\t\t\t}",
                        "\t\t\tret = check_vma_flags(vma, gup_flags);",
                        "\t\t\tif (ret)",
                        "\t\t\t\tgoto out;",
                        "",
                        "\t\t\tif (is_vm_hugetlb_page(vma)) {",
                        "\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,",
                        "\t\t\t\t\t\t&start, &nr_pages, i,",
                        "\t\t\t\t\t\tgup_flags, locked);",
                        "\t\t\t\tif (locked && *locked == 0) {",
                        "\t\t\t\t\t/*",
                        "\t\t\t\t\t * We've got a VM_FAULT_RETRY",
                        "\t\t\t\t\t * and we've lost mmap_lock.",
                        "\t\t\t\t\t * We must stop here.",
                        "\t\t\t\t\t */",
                        "\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);",
                        "\t\t\t\t\tBUG_ON(ret != 0);",
                        "\t\t\t\t\tgoto out;",
                        "\t\t\t\t}",
                        "\t\t\t\tcontinue;",
                        "\t\t\t}",
                        "\t\t}",
                        "retry:",
                        "\t\t/*",
                        "\t\t * If we have a pending SIGKILL, don't keep faulting pages and",
                        "\t\t * potentially allocating memory.",
                        "\t\t */",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tret = -EINTR;",
                        "\t\t\tgoto out;",
                        "\t\t}",
                        "\t\tcond_resched();",
                        "",
                        "\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);",
                        "\t\tif (!page) {",
                        "\t\t\tret = faultin_page(vma, start, &foll_flags, locked);",
                        "\t\t\tswitch (ret) {",
                        "\t\t\tcase 0:",
                        "\t\t\t\tgoto retry;",
                        "\t\t\tcase -EBUSY:",
                        "\t\t\t\tret = 0;",
                        "\t\t\t\tfallthrough;",
                        "\t\t\tcase -EFAULT:",
                        "\t\t\tcase -ENOMEM:",
                        "\t\t\tcase -EHWPOISON:",
                        "\t\t\t\tgoto out;",
                        "\t\t\tcase -ENOENT:",
                        "\t\t\t\tgoto next_page;",
                        "\t\t\t}",
                        "\t\t\tBUG();",
                        "\t\t} else if (PTR_ERR(page) == -EEXIST) {",
                        "\t\t\t/*",
                        "\t\t\t * Proper page table entry exists, but no corresponding",
                        "\t\t\t * struct page.",
                        "\t\t\t */",
                        "\t\t\tgoto next_page;",
                        "\t\t} else if (IS_ERR(page)) {",
                        "\t\t\tret = PTR_ERR(page);",
                        "\t\t\tgoto out;",
                        "\t\t}",
                        "\t\tif (pages) {",
                        "\t\t\tpages[i] = page;",
                        "\t\t\tflush_anon_page(vma, page, start);",
                        "\t\t\tflush_dcache_page(page);",
                        "\t\t\tctx.page_mask = 0;",
                        "\t\t}",
                        "next_page:",
                        "\t\tif (vmas) {",
                        "\t\t\tvmas[i] = vma;",
                        "\t\t\tctx.page_mask = 0;",
                        "\t\t}",
                        "\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);",
                        "\t\tif (page_increm > nr_pages)",
                        "\t\t\tpage_increm = nr_pages;",
                        "\t\ti += page_increm;",
                        "\t\tstart += page_increm * PAGE_SIZE;",
                        "\t\tnr_pages -= page_increm;",
                        "\t} while (nr_pages);",
                        "out:",
                        "\tif (ctx.pgmap)",
                        "\t\tput_dev_pagemap(ctx.pgmap);",
                        "\treturn i ? i : ret;",
                        "}"
                    ],
                    "start": 1026,
                    "highlight": 1137
                }
            ],
            "ins_idx": 426,
            "addr": "0xffffffff811a225f"
        },
        "3872": {
            "name": "+0x23",
            "parent_idx": 3868,
            "source_line": [
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n1090",
                    "code": [
                        "/**",
                        " * __get_user_pages() - pin user pages in memory",
                        " * @mm:\t\tmm_struct of target mm",
                        " * @start:\tstarting user address",
                        " * @nr_pages:\tnumber of pages from start to pin",
                        " * @gup_flags:\tflags modifying pin behaviour",
                        " * @pages:\tarray that receives pointers to the pages pinned.",
                        " *\t\tShould be at least nr_pages long. Or NULL, if caller",
                        " *\t\tonly intends to ensure the pages are faulted in.",
                        " * @vmas:\tarray of pointers to vmas corresponding to each page.",
                        " *\t\tOr NULL if the caller does not require them.",
                        " * @locked:     whether we're still with the mmap_lock held",
                        " *",
                        " * Returns either number of pages pinned (which may be less than the",
                        " * number requested), or an error. Details about the return value:",
                        " *",
                        " * -- If nr_pages is 0, returns 0.",
                        " * -- If nr_pages is >0, but no pages were pinned, returns -errno.",
                        " * -- If nr_pages is >0, and some pages were pinned, returns the number of",
                        " *    pages pinned. Again, this may be less than nr_pages.",
                        " * -- 0 return value is possible when the fault would need to be retried.",
                        " *",
                        " * The caller is responsible for releasing returned @pages, via put_page().",
                        " *",
                        " * @vmas are valid only as long as mmap_lock is held.",
                        " *",
                        " * Must be called with mmap_lock held.  It may be released.  See below.",
                        " *",
                        " * __get_user_pages walks a process's page tables and takes a reference to",
                        " * each struct page that each user address corresponds to at a given",
                        " * instant. That is, it takes the page that would be accessed if a user",
                        " * thread accesses the given user virtual address at that instant.",
                        " *",
                        " * This does not guarantee that the page exists in the user mappings when",
                        " * __get_user_pages returns, and there may even be a completely different",
                        " * page there in some cases (eg. if mmapped pagecache has been invalidated",
                        " * and subsequently re faulted). However it does guarantee that the page",
                        " * won't be freed completely. And mostly callers simply care that the page",
                        " * contains data that was valid *at some point in time*. Typically, an IO",
                        " * or similar operation cannot guarantee anything stronger anyway because",
                        " * locks can't be held over the syscall boundary.",
                        " *",
                        " * If @gup_flags & FOLL_WRITE == 0, the page must not be written to. If",
                        " * the page is written to, set_page_dirty (or set_page_dirty_lock, as",
                        " * appropriate) must be called after the page is finished with, and",
                        " * before put_page is called.",
                        " *",
                        " * If @locked != NULL, *@locked will be set to 0 when mmap_lock is",
                        " * released by an up_read().  That can happen if @gup_flags does not",
                        " * have FOLL_NOWAIT.",
                        " *",
                        " * A caller using such a combination of @locked and @gup_flags",
                        " * must therefore hold the mmap_lock for reading only, and recognize",
                        " * when it's been released.  Otherwise, it must be held for either",
                        " * reading or writing and will not be released.",
                        " *",
                        " * In most cases, get_user_pages or get_user_pages_fast should be used",
                        " * instead of __get_user_pages. __get_user_pages should be used only if",
                        " * you need some special @gup_flags.",
                        " */",
                        "static long __get_user_pages(struct mm_struct *mm,",
                        "\t\tunsigned long start, unsigned long nr_pages,",
                        "\t\tunsigned int gup_flags, struct page **pages,",
                        "\t\tstruct vm_area_struct **vmas, int *locked)",
                        "{",
                        "\tlong ret = 0, i = 0;",
                        "\tstruct vm_area_struct *vma = NULL;",
                        "\tstruct follow_page_context ctx = { NULL };",
                        "",
                        "\tif (!nr_pages)",
                        "\t\treturn 0;",
                        "",
                        "\tstart = untagged_addr(start);",
                        "",
                        "\tVM_BUG_ON(!!pages != !!(gup_flags & (FOLL_GET | FOLL_PIN)));",
                        "",
                        "\t/*",
                        "\t * If FOLL_FORCE is set then do not force a full fault as the hinting",
                        "\t * fault information is unrelated to the reference behaviour of a task",
                        "\t * using the address space",
                        "\t */",
                        "\tif (!(gup_flags & FOLL_FORCE))",
                        "\t\tgup_flags |= FOLL_NUMA;",
                        "",
                        "\tdo {",
                        "\t\tstruct page *page;",
                        "\t\tunsigned int foll_flags = gup_flags;",
                        "\t\tunsigned int page_increm;",
                        "",
                        "\t\t/* first iteration or cross vma bound */",
                        "\t\tif (!vma || start >= vma->vm_end) {",
                        "\t\t\tvma = find_extend_vma(mm, start);",
                        "\t\t\tif (!vma && in_gate_area(mm, start)) {",
                        "\t\t\t\tret = get_gate_page(mm, start & PAGE_MASK,",
                        "\t\t\t\t\t\tgup_flags, &vma,",
                        "\t\t\t\t\t\tpages ? &pages[i] : NULL);",
                        "\t\t\t\tif (ret)",
                        "\t\t\t\t\tgoto out;",
                        "\t\t\t\tctx.page_mask = 0;",
                        "\t\t\t\tgoto next_page;",
                        "\t\t\t}",
                        "",
                        "\t\t\tif (!vma) {",
                        "\t\t\t\tret = -EFAULT;",
                        "\t\t\t\tgoto out;",
                        "\t\t\t}",
                        "\t\t\tret = check_vma_flags(vma, gup_flags);",
                        "\t\t\tif (ret)",
                        "\t\t\t\tgoto out;",
                        "",
                        "\t\t\tif (is_vm_hugetlb_page(vma)) {",
                        "\t\t\t\ti = follow_hugetlb_page(mm, vma, pages, vmas,",
                        "\t\t\t\t\t\t&start, &nr_pages, i,",
                        "\t\t\t\t\t\tgup_flags, locked);",
                        "\t\t\t\tif (locked && *locked == 0) {",
                        "\t\t\t\t\t/*",
                        "\t\t\t\t\t * We've got a VM_FAULT_RETRY",
                        "\t\t\t\t\t * and we've lost mmap_lock.",
                        "\t\t\t\t\t * We must stop here.",
                        "\t\t\t\t\t */",
                        "\t\t\t\t\tBUG_ON(gup_flags & FOLL_NOWAIT);",
                        "\t\t\t\t\tBUG_ON(ret != 0);",
                        "\t\t\t\t\tgoto out;",
                        "\t\t\t\t}",
                        "\t\t\t\tcontinue;",
                        "\t\t\t}",
                        "\t\t}",
                        "retry:",
                        "\t\t/*",
                        "\t\t * If we have a pending SIGKILL, don't keep faulting pages and",
                        "\t\t * potentially allocating memory.",
                        "\t\t */",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tret = -EINTR;",
                        "\t\t\tgoto out;",
                        "\t\t}",
                        "\t\tcond_resched();",
                        "",
                        "\t\tpage = follow_page_mask(vma, start, foll_flags, &ctx);",
                        "\t\tif (!page) {",
                        "\t\t\tret = faultin_page(vma, start, &foll_flags, locked);",
                        "\t\t\tswitch (ret) {",
                        "\t\t\tcase 0:",
                        "\t\t\t\tgoto retry;",
                        "\t\t\tcase -EBUSY:",
                        "\t\t\t\tret = 0;",
                        "\t\t\t\tfallthrough;",
                        "\t\t\tcase -EFAULT:",
                        "\t\t\tcase -ENOMEM:",
                        "\t\t\tcase -EHWPOISON:",
                        "\t\t\t\tgoto out;",
                        "\t\t\tcase -ENOENT:",
                        "\t\t\t\tgoto next_page;",
                        "\t\t\t}",
                        "\t\t\tBUG();",
                        "\t\t} else if (PTR_ERR(page) == -EEXIST) {",
                        "\t\t\t/*",
                        "\t\t\t * Proper page table entry exists, but no corresponding",
                        "\t\t\t * struct page.",
                        "\t\t\t */",
                        "\t\t\tgoto next_page;",
                        "\t\t} else if (IS_ERR(page)) {",
                        "\t\t\tret = PTR_ERR(page);",
                        "\t\t\tgoto out;",
                        "\t\t}",
                        "\t\tif (pages) {",
                        "\t\t\tpages[i] = page;",
                        "\t\t\tflush_anon_page(vma, page, start);",
                        "\t\t\tflush_dcache_page(page);",
                        "\t\t\tctx.page_mask = 0;",
                        "\t\t}",
                        "next_page:",
                        "\t\tif (vmas) {",
                        "\t\t\tvmas[i] = vma;",
                        "\t\t\tctx.page_mask = 0;",
                        "\t\t}",
                        "\t\tpage_increm = 1 + (~(start >> PAGE_SHIFT) & ctx.page_mask);",
                        "\t\tif (page_increm > nr_pages)",
                        "\t\t\tpage_increm = nr_pages;",
                        "\t\ti += page_increm;",
                        "\t\tstart += page_increm * PAGE_SIZE;",
                        "\t\tnr_pages -= page_increm;",
                        "\t} while (nr_pages);",
                        "out:",
                        "\tif (ctx.pgmap)",
                        "\t\tput_dev_pagemap(ctx.pgmap);",
                        "\treturn i ? i : ret;",
                        "}"
                    ],
                    "start": 1026,
                    "highlight": 1090
                }
            ],
            "ins_idx": 567,
            "addr": "0xffffffff811a2153"
        },
        "3866": {
            "name": "+0x9b",
            "parent_idx": 3857,
            "source_line": [
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n1352",
                    "code": [
                        "/*",
                        " * Please note that this function, unlike __get_user_pages will not",
                        " * return 0 for nr_pages > 0 without FOLL_NOWAIT",
                        " */",
                        "static __always_inline long __get_user_pages_locked(struct mm_struct *mm,",
                        "\t\t\t\t\t\tunsigned long start,",
                        "\t\t\t\t\t\tunsigned long nr_pages,",
                        "\t\t\t\t\t\tstruct page **pages,",
                        "\t\t\t\t\t\tstruct vm_area_struct **vmas,",
                        "\t\t\t\t\t\tint *locked,",
                        "\t\t\t\t\t\tunsigned int flags)",
                        "{",
                        "\tlong ret, pages_done;",
                        "\tbool lock_dropped;",
                        "",
                        "\tif (locked) {",
                        "\t\t/* if VM_FAULT_RETRY can be returned, vmas become invalid */",
                        "\t\tBUG_ON(vmas);",
                        "\t\t/* check caller initialized locked */",
                        "\t\tBUG_ON(*locked != 1);",
                        "\t}",
                        "",
                        "\tif (flags & FOLL_PIN)",
                        "\t\tmm_set_has_pinned_flag(&mm->flags);",
                        "",
                        "\t/*",
                        "\t * FOLL_PIN and FOLL_GET are mutually exclusive. Traditional behavior",
                        "\t * is to set FOLL_GET if the caller wants pages[] filled in (but has",
                        "\t * carelessly failed to specify FOLL_GET), so keep doing that, but only",
                        "\t * for FOLL_GET, not for the newer FOLL_PIN.",
                        "\t *",
                        "\t * FOLL_PIN always expects pages to be non-null, but no need to assert",
                        "\t * that here, as any failures will be obvious enough.",
                        "\t */",
                        "\tif (pages && !(flags & FOLL_PIN))",
                        "\t\tflags |= FOLL_GET;",
                        "",
                        "\tpages_done = 0;",
                        "\tlock_dropped = false;",
                        "\tfor (;;) {",
                        "\t\tret = __get_user_pages(mm, start, nr_pages, flags, pages,",
                        "\t\t\t\t       vmas, locked);",
                        "\t\tif (!locked)",
                        "\t\t\t/* VM_FAULT_RETRY couldn't trigger, bypass */",
                        "\t\t\treturn ret;",
                        "",
                        "\t\t/* VM_FAULT_RETRY cannot return errors */",
                        "\t\tif (!*locked) {",
                        "\t\t\tBUG_ON(ret < 0);",
                        "\t\t\tBUG_ON(ret >= nr_pages);",
                        "\t\t}",
                        "",
                        "\t\tif (ret > 0) {",
                        "\t\t\tnr_pages -= ret;",
                        "\t\t\tpages_done += ret;",
                        "\t\t\tif (!nr_pages)",
                        "\t\t\t\tbreak;",
                        "\t\t}",
                        "\t\tif (*locked) {",
                        "\t\t\t/*",
                        "\t\t\t * VM_FAULT_RETRY didn't trigger or it was a",
                        "\t\t\t * FOLL_NOWAIT.",
                        "\t\t\t */",
                        "\t\t\tif (!pages_done)",
                        "\t\t\t\tpages_done = ret;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "\t\t/*",
                        "\t\t * VM_FAULT_RETRY triggered, so seek to the faulting offset.",
                        "\t\t * For the prefault case (!pages) we only update counts.",
                        "\t\t */",
                        "\t\tif (likely(pages))",
                        "\t\t\tpages += ret;",
                        "\t\tstart += ret << PAGE_SHIFT;",
                        "\t\tlock_dropped = true;",
                        "",
                        "retry:",
                        "\t\t/*",
                        "\t\t * Repeat on the address that fired VM_FAULT_RETRY",
                        "\t\t * with both FAULT_FLAG_ALLOW_RETRY and",
                        "\t\t * FAULT_FLAG_TRIED.  Note that GUP can be interrupted",
                        "\t\t * by fatal signals, so we need to check it before we",
                        "\t\t * start trying again otherwise it can loop forever.",
                        "\t\t */",
                        "",
                        "\t\tif (fatal_signal_pending(current)) {",
                        "\t\t\tif (!pages_done)",
                        "\t\t\t\tpages_done = -EINTR;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\tret = mmap_read_lock_killable(mm);",
                        "\t\tif (ret) {",
                        "\t\t\tBUG_ON(ret > 0);",
                        "\t\t\tif (!pages_done)",
                        "\t\t\t\tpages_done = ret;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "",
                        "\t\t*locked = 1;",
                        "\t\tret = __get_user_pages(mm, start, 1, flags | FOLL_TRIED,",
                        "\t\t\t\t       pages, NULL, locked);",
                        "\t\tif (!*locked) {",
                        "\t\t\t/* Continue to retry until we succeeded */",
                        "\t\t\tBUG_ON(ret != 0);",
                        "\t\t\tgoto retry;",
                        "\t\t}",
                        "\t\tif (ret != 1) {",
                        "\t\t\tBUG_ON(ret > 1);",
                        "\t\t\tif (!pages_done)",
                        "\t\t\t\tpages_done = ret;",
                        "\t\t\tbreak;",
                        "\t\t}",
                        "\t\tnr_pages--;",
                        "\t\tpages_done++;",
                        "\t\tif (!nr_pages)",
                        "\t\t\tbreak;",
                        "\t\tif (likely(pages))",
                        "\t\t\tpages++;",
                        "\t\tstart += PAGE_SIZE;",
                        "\t}",
                        "\tif (lock_dropped && *locked) {",
                        "\t\t/*",
                        "\t\t * We must let the caller know we temporarily dropped the lock",
                        "\t\t * and so the critical section protected by it was lost.",
                        "\t\t */",
                        "\t\tmmap_read_unlock(mm);",
                        "\t\t*locked = 0;",
                        "\t}",
                        "\treturn pages_done;",
                        "}"
                    ],
                    "start": 1312,
                    "highlight": 1352
                },
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n1745",
                    "code": [
                        "/*",
                        " * __gup_longterm_locked() is a wrapper for __get_user_pages_locked which",
                        " * allows us to process the FOLL_LONGTERM flag.",
                        " */",
                        "static long __gup_longterm_locked(struct mm_struct *mm,",
                        "\t\t\t\t  unsigned long start,",
                        "\t\t\t\t  unsigned long nr_pages,",
                        "\t\t\t\t  struct page **pages,",
                        "\t\t\t\t  struct vm_area_struct **vmas,",
                        "\t\t\t\t  unsigned int gup_flags)",
                        "{",
                        "\tunsigned int flags;",
                        "\tlong rc;",
                        "",
                        "\tif (!(gup_flags & FOLL_LONGTERM))",
                        "\t\treturn __get_user_pages_locked(mm, start, nr_pages, pages, vmas,",
                        "\t\t\t\t\t       NULL, gup_flags);",
                        "\tflags = memalloc_pin_save();",
                        "\tdo {",
                        "\t\trc = __get_user_pages_locked(mm, start, nr_pages, pages, vmas,",
                        "\t\t\t\t\t     NULL, gup_flags);",
                        "\t\tif (rc <= 0)",
                        "\t\t\tbreak;",
                        "\t\trc = check_and_migrate_movable_pages(rc, pages, gup_flags);",
                        "\t} while (!rc);",
                        "\tmemalloc_pin_restore(flags);",
                        "",
                        "\treturn rc;",
                        "}"
                    ],
                    "start": 1726,
                    "highlight": 1745
                }
            ],
            "ins_idx": 736,
            "addr": "0xffffffff811a327b"
        },
        "3860": {
            "name": "+0xd",
            "parent_idx": 3857,
            "source_line": [
                {
                    "file": "mm/gup.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/mm/gup.c?id=3dbdb38e#n1736",
                    "code": [
                        "/*",
                        " * __gup_longterm_locked() is a wrapper for __get_user_pages_locked which",
                        " * allows us to process the FOLL_LONGTERM flag.",
                        " */",
                        "static long __gup_longterm_locked(struct mm_struct *mm,",
                        "\t\t\t\t  unsigned long start,",
                        "\t\t\t\t  unsigned long nr_pages,",
                        "\t\t\t\t  struct page **pages,",
                        "\t\t\t\t  struct vm_area_struct **vmas,",
                        "\t\t\t\t  unsigned int gup_flags)",
                        "{",
                        "\tunsigned int flags;",
                        "\tlong rc;",
                        "",
                        "\tif (!(gup_flags & FOLL_LONGTERM))",
                        "\t\treturn __get_user_pages_locked(mm, start, nr_pages, pages, vmas,",
                        "\t\t\t\t\t       NULL, gup_flags);",
                        "\tflags = memalloc_pin_save();",
                        "\tdo {",
                        "\t\trc = __get_user_pages_locked(mm, start, nr_pages, pages, vmas,",
                        "\t\t\t\t\t     NULL, gup_flags);",
                        "\t\tif (rc <= 0)",
                        "\t\t\tbreak;",
                        "\t\trc = check_and_migrate_movable_pages(rc, pages, gup_flags);",
                        "\t} while (!rc);",
                        "\tmemalloc_pin_restore(flags);",
                        "",
                        "\treturn rc;",
                        "}"
                    ],
                    "start": 1726,
                    "highlight": 1736
                }
            ],
            "ins_idx": 889,
            "addr": "0xffffffff811a31ed"
        },
        "3854": {
            "name": "+0x125",
            "parent_idx": 3834,
            "source_line": [
                {
                    "file": "fs/io_uring.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=3dbdb38e#n8381",
                    "code": [
                        "static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,",
                        "\t\t\t\t  struct io_mapped_ubuf **pimu,",
                        "\t\t\t\t  struct page **last_hpage)",
                        "{",
                        "\tstruct io_mapped_ubuf *imu = NULL;",
                        "\tstruct vm_area_struct **vmas = NULL;",
                        "\tstruct page **pages = NULL;",
                        "\tunsigned long off, start, end, ubuf;",
                        "\tsize_t size;",
                        "\tint ret, pret, nr_pages, i;",
                        "",
                        "\tif (!iov->iov_base) {",
                        "\t\t*pimu = ctx->dummy_ubuf;",
                        "\t\treturn 0;",
                        "\t}",
                        "",
                        "\tubuf = (unsigned long) iov->iov_base;",
                        "\tend = (ubuf + iov->iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;",
                        "\tstart = ubuf >> PAGE_SHIFT;",
                        "\tnr_pages = end - start;",
                        "",
                        "\t*pimu = NULL;",
                        "\tret = -ENOMEM;",
                        "",
                        "\tpages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);",
                        "\tif (!pages)",
                        "\t\tgoto done;",
                        "",
                        "\tvmas = kvmalloc_array(nr_pages, sizeof(struct vm_area_struct *),",
                        "\t\t\t      GFP_KERNEL);",
                        "\tif (!vmas)",
                        "\t\tgoto done;",
                        "",
                        "\timu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);",
                        "\tif (!imu)",
                        "\t\tgoto done;",
                        "",
                        "\tret = 0;",
                        "\tmmap_read_lock(current->mm);",
                        "\tpret = pin_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,",
                        "\t\t\t      pages, vmas);",
                        "\tif (pret == nr_pages) {",
                        "\t\t/* don't support file backed memory */",
                        "\t\tfor (i = 0; i < nr_pages; i++) {",
                        "\t\t\tstruct vm_area_struct *vma = vmas[i];",
                        "",
                        "\t\t\tif (vma_is_shmem(vma))",
                        "\t\t\t\tcontinue;",
                        "\t\t\tif (vma->vm_file &&",
                        "\t\t\t    !is_file_hugepages(vma->vm_file)) {",
                        "\t\t\t\tret = -EOPNOTSUPP;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t}",
                        "\t} else {",
                        "\t\tret = pret < 0 ? pret : -EFAULT;",
                        "\t}",
                        "\tmmap_read_unlock(current->mm);",
                        "\tif (ret) {",
                        "\t\t/*",
                        "\t\t * if we did partial map, or found file backed vmas,",
                        "\t\t * release any pages we did get",
                        "\t\t */",
                        "\t\tif (pret > 0)",
                        "\t\t\tunpin_user_pages(pages, pret);",
                        "\t\tgoto done;",
                        "\t}",
                        "",
                        "\tret = io_buffer_account_pin(ctx, pages, pret, imu, last_hpage);",
                        "\tif (ret) {",
                        "\t\tunpin_user_pages(pages, pret);",
                        "\t\tgoto done;",
                        "\t}",
                        "",
                        "\toff = ubuf & ~PAGE_MASK;",
                        "\tsize = iov->iov_len;",
                        "\tfor (i = 0; i < nr_pages; i++) {",
                        "\t\tsize_t vec_len;",
                        "",
                        "\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);",
                        "\t\timu->bvec[i].bv_page = pages[i];",
                        "\t\timu->bvec[i].bv_len = vec_len;",
                        "\t\timu->bvec[i].bv_offset = off;",
                        "\t\toff = 0;",
                        "\t\tsize -= vec_len;",
                        "\t}",
                        "\t/* store original address for later verification */",
                        "\timu->ubuf = ubuf;",
                        "\timu->ubuf_end = ubuf + iov->iov_len;",
                        "\timu->nr_bvecs = nr_pages;",
                        "\t*pimu = imu;",
                        "\tret = 0;",
                        "done:",
                        "\tif (ret)",
                        "\t\tkvfree(imu);",
                        "\tkvfree(pages);",
                        "\tkvfree(vmas);",
                        "\treturn ret;",
                        "}"
                    ],
                    "start": 8342,
                    "highlight": 8381
                }
            ],
            "ins_idx": 1084,
            "addr": "0xffffffff81248725"
        },
        "3847": {
            "name": "+0x7d",
            "parent_idx": 3834,
            "source_line": [
                {
                    "file": "./include/linux/mm.h",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/./include/linux/mm.h?id=3dbdb38e#n798",
                    "code": [
                        "extern void *kvmalloc_node(size_t size, gfp_t flags, int node);",
                        "static inline void *kvmalloc(size_t size, gfp_t flags)",
                        "{",
                        "\treturn kvmalloc_node(size, flags, NUMA_NO_NODE);",
                        "}"
                    ],
                    "start": 795,
                    "highlight": 798
                },
                {
                    "file": "./include/linux/mm.h",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/./include/linux/mm.h?id=3dbdb38e#n816",
                    "code": [
                        "static inline void *kvmalloc_array(size_t n, size_t size, gfp_t flags)",
                        "{",
                        "\tsize_t bytes;",
                        "",
                        "\tif (unlikely(check_mul_overflow(n, size, &bytes)))",
                        "\t\treturn NULL;",
                        "",
                        "\treturn kvmalloc(bytes, flags);",
                        "}"
                    ],
                    "start": 809,
                    "highlight": 816
                },
                {
                    "file": "fs/io_uring.c",
                    "url": "https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/fs/io_uring.c?id=3dbdb38e#n8366",
                    "code": [
                        "static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,",
                        "\t\t\t\t  struct io_mapped_ubuf **pimu,",
                        "\t\t\t\t  struct page **last_hpage)",
                        "{",
                        "\tstruct io_mapped_ubuf *imu = NULL;",
                        "\tstruct vm_area_struct **vmas = NULL;",
                        "\tstruct page **pages = NULL;",
                        "\tunsigned long off, start, end, ubuf;",
                        "\tsize_t size;",
                        "\tint ret, pret, nr_pages, i;",
                        "",
                        "\tif (!iov->iov_base) {",
                        "\t\t*pimu = ctx->dummy_ubuf;",
                        "\t\treturn 0;",
                        "\t}",
                        "",
                        "\tubuf = (unsigned long) iov->iov_base;",
                        "\tend = (ubuf + iov->iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;",
                        "\tstart = ubuf >> PAGE_SHIFT;",
                        "\tnr_pages = end - start;",
                        "",
                        "\t*pimu = NULL;",
                        "\tret = -ENOMEM;",
                        "",
                        "\tpages = kvmalloc_array(nr_pages, sizeof(struct page *), GFP_KERNEL);",
                        "\tif (!pages)",
                        "\t\tgoto done;",
                        "",
                        "\tvmas = kvmalloc_array(nr_pages, sizeof(struct vm_area_struct *),",
                        "\t\t\t      GFP_KERNEL);",
                        "\tif (!vmas)",
                        "\t\tgoto done;",
                        "",
                        "\timu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);",
                        "\tif (!imu)",
                        "\t\tgoto done;",
                        "",
                        "\tret = 0;",
                        "\tmmap_read_lock(current->mm);",
                        "\tpret = pin_user_pages(ubuf, nr_pages, FOLL_WRITE | FOLL_LONGTERM,",
                        "\t\t\t      pages, vmas);",
                        "\tif (pret == nr_pages) {",
                        "\t\t/* don't support file backed memory */",
                        "\t\tfor (i = 0; i < nr_pages; i++) {",
                        "\t\t\tstruct vm_area_struct *vma = vmas[i];",
                        "",
                        "\t\t\tif (vma_is_shmem(vma))",
                        "\t\t\t\tcontinue;",
                        "\t\t\tif (vma->vm_file &&",
                        "\t\t\t    !is_file_hugepages(vma->vm_file)) {",
                        "\t\t\t\tret = -EOPNOTSUPP;",
                        "\t\t\t\tbreak;",
                        "\t\t\t}",
                        "\t\t}",
                        "\t} else {",
                        "\t\tret = pret < 0 ? pret : -EFAULT;",
                        "\t}",
                        "\tmmap_read_unlock(current->mm);",
                        "\tif (ret) {",
                        "\t\t/*",
                        "\t\t * if we did partial map, or found file backed vmas,",
                        "\t\t * release any pages we did get",
                        "\t\t */",
                        "\t\tif (pret > 0)",
                        "\t\t\tunpin_user_pages(pages, pret);",
                        "\t\tgoto done;",
                        "\t}",
                        "",
                        "\tret = io_buffer_account_pin(ctx, pages, pret, imu, last_hpage);",
                        "\tif (ret) {",
                        "\t\tunpin_user_pages(pages, pret);",
                        "\t\tgoto done;",
                        "\t}",
                        "",
                        "\toff = ubuf & ~PAGE_MASK;",
                        "\tsize = iov->iov_len;",
                        "\tfor (i = 0; i < nr_pages; i++) {",
                        "\t\tsize_t vec_len;",
                        "",
                        "\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);",
                        "\t\timu->bvec[i].bv_page = pages[i];",
                        "\t\timu->bvec[i].bv_len = vec_len;",
                        "\t\timu->bvec[i].bv_offset = off;",
                        "\t\toff = 0;",
                        "\t\tsize -= vec_len;",
                        "\t}",
                        "\t/* store original address for later verification */",
                        "\timu->ubuf = ubuf;",
                        "\timu->ubuf_end = ubuf + iov->iov_len;",
                        "\timu->nr_bvecs = nr_pages;",
                        "\t*pimu = imu;",
                        "\tret = 0;",
                        "done:",
                        "\tif (ret)",
                        "\t\tkvfree(imu);",
                        "\tkvfree(pages);",
                        "\tkvfree(vmas);",
                        "\treturn ret;",
                        "}"
                    ],
                    "start": 8342,
                    "highlight": 8366
                }
            ],
            "ins_idx": 1564,
            "addr": "0xffffffff8124867d"
        }
    },
    "ins": {
        "51": {
            "name": "mov rax, qword ptr [rdi]",
            "desc": "Invalid Memory Access\nInvalid Base Address 0x0",
            "call_idx": 4405,
            "inputs": [
                8138,
                8139
            ],
            "outputs": [
                8140
            ]
        },
        "136": {
            "name": "mov rdi, qword ptr [rax + r11]",
            "desc": "Heap Use Before Initialization",
            "call_idx": 4403,
            "inputs": [
                8134,
                8135,
                8136
            ],
            "outputs": [
                8137
            ]
        },
        "225": {
            "name": "ret ",
            "desc": "Source of base address MEMALLOC 0x0",
            "call_idx": 3846,
            "inputs": [],
            "outputs": [
                7095
            ]
        },
        "224": {
            "name": "mov rax, qword ptr [rsp + 0x10]",
            "desc": "",
            "call_idx": 4402,
            "inputs": [
                8131,
                8132
            ],
            "outputs": [
                8133
            ]
        },
        "323": {
            "name": "mov qword ptr [rsp + 0x10], rdx",
            "desc": "",
            "call_idx": 3906,
            "inputs": [
                7208,
                7209
            ],
            "outputs": [
                7210
            ]
        },
        "426": {
            "name": "mov rdx, qword ptr [rsp + 0x28]",
            "desc": "",
            "call_idx": 3899,
            "inputs": [
                7194,
                7195
            ],
            "outputs": [
                7196
            ]
        },
        "567": {
            "name": "mov qword ptr [rsp + 0x10], r8",
            "desc": "",
            "call_idx": 3872,
            "inputs": [
                7141,
                7142
            ],
            "outputs": [
                7143
            ]
        },
        "736": {
            "name": "mov r8, rbx",
            "desc": "",
            "call_idx": 3866,
            "inputs": [
                7129
            ],
            "outputs": [
                7130
            ]
        },
        "889": {
            "name": "mov rbx, rcx",
            "desc": "",
            "call_idx": 3860,
            "inputs": [
                7116
            ],
            "outputs": [
                7117
            ]
        },
        "1084": {
            "name": "mov rcx, rbp",
            "desc": "",
            "call_idx": 3854,
            "inputs": [
                7106
            ],
            "outputs": [
                7107
            ]
        },
        "1564": {
            "name": "mov rbp, rax",
            "desc": "",
            "call_idx": 3847,
            "inputs": [
                7096
            ],
            "outputs": [
                7097
            ]
        }
    },
    "data": {
        "8138": {
            "name": "RDI",
            "ins_idx": 51,
            "value": "0x0",
            "sources": []
        },
        "8139": {
            "name": "[0x0]",
            "ins_idx": 51,
            "value": "0x0",
            "sources": []
        },
        "8140": {
            "name": "RAX",
            "ins_idx": 51,
            "value": "0x0",
            "sources": []
        },
        "8134": {
            "name": "R11",
            "ins_idx": 136,
            "value": "0x3ff8",
            "sources": []
        },
        "8135": {
            "name": "RAX",
            "ins_idx": 136,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "8136": {
            "name": "[0xffff888101d83ff8]",
            "ins_idx": 136,
            "value": "0x0",
            "sources": []
        },
        "8137": {
            "name": "RDI",
            "ins_idx": 136,
            "value": "0x0",
            "sources": [
                8138
            ]
        },
        "7095": {
            "name": "RAX",
            "ins_idx": 225,
            "value": "0xffff888101d80000",
            "sources": [
                8135,
                7096
            ]
        },
        "8131": {
            "name": "RSP",
            "ins_idx": 224,
            "value": "0xffffc9000025fbd8",
            "sources": []
        },
        "8132": {
            "name": "[0xffffc9000025fbe8]",
            "ins_idx": 224,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "8133": {
            "name": "RAX",
            "ins_idx": 224,
            "value": "0xffff888101d80000",
            "sources": [
                8135
            ]
        },
        "7208": {
            "name": "RSP",
            "ins_idx": 323,
            "value": "0xffffc9000025fbd8",
            "sources": []
        },
        "7209": {
            "name": "RDX",
            "ins_idx": 323,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "7210": {
            "name": "[0xffffc9000025fbe8]",
            "ins_idx": 323,
            "value": "0xffff888101d80000",
            "sources": [
                8132
            ]
        },
        "7194": {
            "name": "RSP",
            "ins_idx": 426,
            "value": "0xffffc9000025fc48",
            "sources": []
        },
        "7195": {
            "name": "[0xffffc9000025fc70]",
            "ins_idx": 426,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "7196": {
            "name": "RDX",
            "ins_idx": 426,
            "value": "0xffff888101d80000",
            "sources": [
                7209
            ]
        },
        "7141": {
            "name": "RSP",
            "ins_idx": 567,
            "value": "0xffffc9000025fc60",
            "sources": []
        },
        "7142": {
            "name": "R8",
            "ins_idx": 567,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "7143": {
            "name": "[0xffffc9000025fc70]",
            "ins_idx": 567,
            "value": "0xffff888101d80000",
            "sources": [
                7195
            ]
        },
        "7129": {
            "name": "RBX",
            "ins_idx": 736,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "7130": {
            "name": "R8",
            "ins_idx": 736,
            "value": "0xffff888101d80000",
            "sources": [
                7142
            ]
        },
        "7116": {
            "name": "RCX",
            "ins_idx": 889,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "7117": {
            "name": "RBX",
            "ins_idx": 889,
            "value": "0xffff888101d80000",
            "sources": [
                7129
            ]
        },
        "7106": {
            "name": "RBP",
            "ins_idx": 1084,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "7107": {
            "name": "RCX",
            "ins_idx": 1084,
            "value": "0xffff888101d80000",
            "sources": [
                7116
            ]
        },
        "7096": {
            "name": "RAX",
            "ins_idx": 1564,
            "value": "0xffff888101d80000",
            "sources": []
        },
        "7097": {
            "name": "RBP",
            "ins_idx": 1564,
            "value": "0xffff888101d80000",
            "sources": [
                7106
            ]
        }
    },
    "chain": {
        "51": [
            136
        ],
        "136": [
            225,
            224
        ],
        "224": [
            323
        ],
        "323": [
            426
        ],
        "426": [
            567
        ],
        "567": [
            736
        ],
        "736": [
            889
        ],
        "889": [
            1084
        ],
        "1084": [
            1564
        ],
        "1564": [
            225
        ]
    }
}